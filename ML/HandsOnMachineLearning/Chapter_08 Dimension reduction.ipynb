{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 차원의 저주\n",
    "\n",
    "데이터의 차원이 증가할수록 데이터 포인트 간의 거리 또한 증가하게 된다.\n",
    "\n",
    "이러한 데이터를 이용해 머신러닝 알고리즘을 학습 하게되면 모델이 복잡해지게 된다. 따라서, `오버피팅(overfitting)` 위험이 커진다. \n",
    "\n",
    "이러한 차원의 저주를 해결하기 위한 방법 중 하나는 데이터의 밀도가 높아질 때까지 학습 데이터셋의 크기를 늘리는 것이다. 하지만, 데이터셋의 크기에 비해 차원은 기하급수적으로 커지기 때문에 매우 힘든일이라 할 수 있다.\n",
    "\n",
    "### 8.2 차원 축소를 위한 접근 방법\n",
    "\n",
    "#### 8.2.1 투영\n",
    "\n",
    "<img src = '../img/3d-data.png' width = \"350\" style=\"float: left\">\n",
    "\n",
    "<img src = '../img/2d-data.png' width = \"300\" style=\"float: right\">\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "- &nbsp;&nbsp; 투영 : 고차원 공간에서 저차원 **부분 공간(subspace)**으로 위치시키는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **투영의 문제점**\n",
    "\n",
    "<img src = '../img/swiss_roll.png' width = \"300\" style=\"float: left\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- &nbsp;&nbsp; 다음과 같은 그림을 평면에 투영시키면 아래 그림의 왼쪽과 같이 나와 구분이 더 어려워진다. \n",
    "- &nbsp;&nbsp; 이상적인 분류 방법은 오른쪽과 같다.\n",
    "\n",
    "<img src = '../img/swiss_roll02.png' width = \"450\" style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2 매니폴드 학습\n",
    "\n",
    "위의 그림은 스위스 롤로 **2D의 매니폴드**의 한 예이다.\n",
    "\n",
    "대부분의 차원 축소 알고리즘이 이러한 **매니폴드**를 모델링하는 방식으로 동작하며, 이를 **매니폴드 학습(Manifold Learning)** 이라고 한다. 매니폴드 학습은 **매니폴드 가정**(manifold assumption) 또는 **매니폴드 가설**(manifold hypothesis)에 의해, 고차원인 실제 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다고 가정한다.\n",
    "\n",
    "따라서, 모델을 학습시키기 전에 학습 데이터셋의 차원을 감소시키면 학습 속도는 빨라지지만 모델의 성능은 항상 더 낫거나 간단한 모델이 되는 것은 아니다. 이것은 데이터셋이 어떠한 모양을 하고 있느냐에 따라 달라진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 PCA\n",
    "\n",
    "**주성분 분석(PCA**, Principal Component Analysis**)**은 가장 대표적인 차원 축소 알고리즘이다. PCA는 먼저 데이터에 가장 가까운 초평면(hyperplane)을 구한 다음, 데이터를 이 초평면에 투영(projection)시킨다.\n",
    "\n",
    "<img src = '../img/pca01.png' width = \"450\" style=\"float: right\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### 8.3.1 분산 보존\n",
    "\n",
    "- PCA는 데이터의 분산이 최대가 되는 축을 찾는다. 즉, 원본 데이터셋과 투영된 데이터셋 간의 **평균제곱거리**를 **최소화** 하는 축을 찾는다.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "#### 8.3.2 주성분\n",
    "\n",
    "PCA는 다음과 같은 단계로 이루어진다. \n",
    "\n",
    "> 1. 학습 데이터셋에서 **분산이 최대**인 `축(axis)`을 찾는다. \n",
    "> 2. 이렇게 찾은 **첫번째 축과 직교(orthogonal)**하면서 **분산이 최대인 두 번째 축**을 찾는다.\n",
    "> 3. 첫 번째 축과 두 번째 축에 **직교하고 분산을 최대한 보존**하는 **세 번째 축**을 찾는다.\n",
    "> 4. `1~3`과 같은 방법으로 데이터셋의 차원(특성 수)만큼의 축을 찾는다.\n",
    "\n",
    "#### 8.3.3 d차원으로 투영하기\n",
    "#### 8.3.4 사이킷런 사용하기\n",
    "#### 8.3.5 설명된 분산의 비율\n",
    "#### 8.3.6 적절한 차원 수 선택하기\n",
    "\n",
    "<img src = '../img/8_1.png' width = \"300\" style=\"float: left\">\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "&nbsp;&nbsp; 왼쪽 그림을 볼 때 특징 수가 3 또는 4일 때 **기울기가 급속히 완만**\n",
    "\n",
    "&nbsp;&nbsp; 그래프에서 **변곡점인 특징의 수**를 선택하거나 \n",
    "\n",
    "&nbsp;&nbsp; **설명된 분산이 0.8정도 되었을 때**의 특징의 수를 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.7 압축을 위한 PCA\n",
    "#### 8.3.8 점진적 PCA\n",
    "\n",
    "- 점진적 PCA(Incremental PCA) : 학습 데이터셋을 미니배치로 나눈 뒤 IPCA 알고리즘에 하나씩 미니배치를 입력으로 넣어준다.\n",
    "\n",
    "#### 8.3.9 랜덤 PCA\n",
    "\n",
    "Scikit-Learn의 `PCA`에서는 **Randomized PCA**를 제공한다. 이 방법은 첫 $d$개의 주성분에 대해 근사값을 빠르게 찾는다.\n",
    "\n",
    "### 8.4 커널 PCA\n",
    "\n",
    "<img src = '../img/8_2.png' width = \"550\" style=\"float: right\">\n",
    "\n",
    "- **Kernel PCA**(KPCA) : 비선형 투영으로 차원을 축소하는 것\n",
    "\n",
    "- 여러 가지 커널의 kPCA를 사용해 2D로 축소시킨 스위스 롤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 LLE\n",
    "\n",
    "- 지역 선형 임베딩(Locally Linear Embedding) : **비선형 차원 축소** 기술\n",
    "    - 투영에 의존하지 않는 매니폴드 학습\n",
    "\n",
    "<img src = '../img/LLE.png' width = \"500\" style=\"float: right\">    \n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "- Step\n",
    "    > 1. k개의 가까운 이웃 찾기\n",
    "    > 2. 선형적으로 연관있는지 확인\n",
    "    > 3. 저차원으로 매핑\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 다른 차원 축소 기법\n",
    "\n",
    "<img src = '../img/8_3.png' width = \"600\" style=\"float: right\">\n",
    "\n",
    "- **다차원 스케일링**(MDS): MDS는 데이터 포인트 간의 거리를 보존하면서 차원을 축소하는 기법이다. \n",
    "- **Isomap**: Isomap은 각 데이터 포인트를 가장 가까운 이웃과 연결하는 식의 그래프를 만든 후 그래프에서 두 노드 사이의 최단 경로를 이루는 노드의 수인  **geodesic distance**를 유지 하면서 차원을 축소한다.\n",
    "- **t-SNE**(t-distributed Stochastic Neighbor Embedding): t-SNE는 비슷한 데이터는 가까이, 비슷하지 않은 데이터는 멀리 떨어지도록 차원을 축소한다. 주로 시각화에 많이 사용되며, 특히 고차원 공간에 있는 데이터의 군집을 시각화할 때 사용한다.\n",
    "- **선형 판별 분석**(LDA): LDA는  Supervised learning이며, 분류 알고리즘에 속한다. LDA는 학습 단계에서 클래스를 가장 잘 구분하는 축을 학습하며, 이 축은 데이터가 투영되는 초평면을 정의하는 데 사용할 수 있다. 이러한 초평면으로 데이터를 투영하게 되면 클래스 간의 거리를 멀리 떨어지게 축소할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
