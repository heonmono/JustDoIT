{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 선형 SVM 분류\n",
    "\n",
    "- 라진 마진 분류\n",
    "    - 두 데이터의 클래스를 분리할 수 있는 무수히 많은 직선들 중 두 데이터 클래스간의 간격(margin)이 최대로하는 **MMH**(Maximum Marginal Hyperplane, 최대 마진 초평면)을 찾아 분리하는 방법\n",
    "    <img src = '../img/svm01.png' width = \"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 소프트 마진 분류\n",
    "\n",
    "<img src = '../img/5_1.png' width = \"650\" style=\"float: right\">\n",
    "\n",
    "- 하드 마진 분류\n",
    "    - **매우 엄격하게** 두 개의 클래스를 분리하는 방법\n",
    "    - **모든**샘플이 올바르게 분류\n",
    "    - 문제점\n",
    "        - **선형적**으로 구분될 수 있을때 제대로 작동\n",
    "        - **이상치**에 민감`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard Margin의 위와 같은 문제를 해결하기 위해,**소프트 마진**(Soft Margin) SVM이 개발.\n",
    "\n",
    "소프트 마진 SVM은 기본적으로 하드 마진 방법을 기반으로 하되, Support Vectors가 위치한 경계선에 약간의 **여유 변수**(Slack Variable)을 두어 Margin의 크기와 마진오류(Margin violation)의 Trade-off에서 최적의 균형을 찾는다는 차이가 있다.\n",
    "\n",
    "Scikit-Learn의 SVM 모델에서는 이러한 여유 변수를 `C`라는 하이퍼파라미터를 제공한다. `C`는 일종의 **penalty**라고 볼 수 있다.\n",
    "\n",
    "<img src = '../img/5_2.png' width = \"700\"  style=\"float: right\">\n",
    "\n",
    "- `C` 값을 줄이면 오류를 허용하는 정도가 커지며, Margin 또한 커진다.\n",
    "- `C` 값을 크게하면 오류를 허용하는 정도가 작아지며, Margin 또한 작아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 비선형 SVM 분류\n",
    "\n",
    "<img src = '../img/5_3.png' width = \"600\"  style=\"float: right\">\n",
    "\n",
    "실제 데이터셋은 위에서 살펴본 Linear SVM으로 분류할 수 없는 즉, 선형적으로 분류할 수 없는 비선형적인 데이터셋이 많다. \n",
    "\n",
    "이러한, 비선형 데이터셋을 다루는 한 가지 방법은 다항 특성(polynomial features)과 같은 특성을 추가하는 방법이 있다.\n",
    "\n",
    "- 다항특성을 이용한 비선형 분류의 예는 오른쪽의 그림과 같다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 5.2.1 다항식 커널\n",
    "\n",
    "<img src = '../img/5_5.png' width = \"350\" style=\"float: left\">\n",
    "\n",
    "<img src = '../img/5_4.png' width = \"350\" style=\"float: right\">\n",
    "\n",
    "<br><br><br><br><br>\n",
    "- 다항식 커널을 사용한 **SVM 분류기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 유사도 특성 추가\n",
    "\n",
    "비선형 특성을 다루는 또 다른 방법은 각 데이터(샘플)이 특정 **랜드마크**(landmark)와 얼마나 닮았는지 측정하는 **유사도 함수**(similarity function)로 계산한 특성을 추가하는 것이다. \n",
    "\n",
    "예를 들어, 아래의 그래프와 같이 $x_2 = -2$와 $x_3 = 1$을 랜드마크라고 하고, $\\gamma = 0.3$인 가우시안 **RBF**(Radial Basis Function, 방사 기저 함수)를 유사도 함수라고 정의해보자.\n",
    "\n",
    "$$\n",
    "\\phi_{\\gamma} \\left( \\mathbf{x}, \\ell \\right) = \\text{exp} \\left( -\\gamma \\left\\| \\mathbf{x} - \\ell \\right\\|^{2} \\right)\n",
    "$$\n",
    "\n",
    "<img src = '../img/5_6.png' width = \"600\" style=\"float: right\">\n",
    "\n",
    "예를 들어, $x_1 = -1$일 경우 \n",
    "\n",
    "- 첫 번째 랜드마크인 $x_1 = -2$에서 $\\| -1 - (-2) \\| = 1$ \n",
    "- 두 번째 랜드마크인 $x_2 = 1$에서 $2$\n",
    "\n",
    "따라서, $x_1$의 새로운 특성은 $x_2 = \\text{exp}(-0.3 \\times 1^2) \\approx 0.74$ 와 $x_3 = \\text{exp}(-0.3 \\times 2^2) \\approx 0.30$이다. \n",
    "\n",
    "이러한, 유사도 특성 추가를 통해 아래의 오른쪽 그래프와 같이 선형분리가 가능하다.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 유사도 특성추가의 가장 간단한 방법은 모든 샘플위치에 랜드마크를 설정하는 것인데 모든 샘플 n개를 랜드마크로 설정 하는 것이다. \n",
    "- 단점은 훈련세트에 있는 모든 샘플을 기준으로 특성을 하나씩 추가하는 것이기 때문에  n개 특성을 가진 m개의 샘플이 m개의 특성을 가진 m개의 샘플이 된다는 것 이다(원본 특성 제외). 훈련 세트가 매우 클 경우 동일한 크기의 매우 많은 특성이 만들어지게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 가우시안 RBF 커널\n",
    "\n",
    "유사도 특성을 추가하는 방법도 유용하게 사용될 수 있다. 하지만, 이러한 특성을 추가하기 위해서는 **계산 비용이 많이 드는 문제**가 있다. \n",
    "\n",
    "이를 가우시안 **RBF Kernel**을 이용하면 위와 같이 특성들을 계산하지 않고 비슷한 결과를 얻을 수 있다.\n",
    "\n",
    "$$\n",
    "K \\left( \\mathbf{a}, \\mathbf{b} \\right) = \\text{exp} \\left( -\\gamma \\left\\| \\mathbf{a} - \\mathbf{b} \\right\\|^{2} \\right)\n",
    "$$\n",
    "\n",
    "- $\\gamma$ : regularization 역할을 함\n",
    "    - $\\gamma$가 커지면 종 모양이 좁아져 각 데이터의 영향 범위가 작아져, 결정 경계(Decision Boundary)가 불규칙하고 구부러진다.\n",
    "    - $\\gamma$가 작아지면 넓은 종 모양이 되며, 데이터의 영향이 넓어져 결정 경계가 부드러워 진다.     \n",
    "    \n",
    "<img src = '../img/5_7.png' width = \"600\">\n",
    "\n",
    "#### Tip : 어떤 커널(kernel)을 사용할까?\n",
    "\n",
    "> 가장먼저 선형 커널(linear kernel)을 사용해본다. 특히, Train set이 매우 크거나, 특성 수가 많을 경우에 해당되며, Scikit-Learn에서는 `LinearSVC`가 `SVC(kernel='linear')` 보다 훨씬 빠르다. \n",
    "\n",
    ">Train set이 크지 않다면, 가우시안 RBF 커널을 시도하는 것이 좋고, 대부분의 경우 이 커널이 잘 맞는다고 한다.\n",
    "\n",
    "### 5.3 SVM 회귀\n",
    "\n",
    "SVM은 선형, 비선형 분류 뿐만 아니라 선형, 비선형 회귀(regression)에도 사용할 수 있다. \n",
    "\n",
    "SVM을 회귀에 적용하는 방법은 분류와는 반대라고 할 수 있다. 분류에서는 정해진 margin 오류 안에서 두 클래스 간의 폭(너비)가 가능한 최대가 되도록 하는 것이었다. 하지만, 회귀에서는 마진(margin)의 밖이 오류에 해당하며, 마진 안쪽으로 최대한 많은 데이터들이 포함되도록 학습한다.\n",
    "\n",
    "마진 안쪽으로 Train set이 추가되어도 모델의 예측에는 영향을 주지 않는다.\n",
    "\n",
    "<img src = '../img/5_8.png' width = \"600\" style=\"float: right\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **LinearSVR**을 이용한 선형 SVM 회귀\n",
    "\n",
    "$$\n",
    "y = 4 + 3x + \\text{noise}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../img/5_8.png' width = \"600\" style=\"float: left\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **SVR**을 이용한 비선형 SVM 회귀\n",
    "\n",
    "$$\n",
    "y = 0.2 + 0.1x + 0.5 x^{2} + \\text{noise}\n",
    "$$\n",
    "\n",
    "- SVM을 **`이상치 탐지`**에도 사용가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 SVM 이론\n",
    "\n",
    "    \n",
    "#### 커널 SVM\n",
    "원공간(Input Space)의 데이터를 선형분류가 가능한 고차원 공간(Feature Space)으로 매핑한 뒤 두 범주를 분류하는 초평면을 찾는 기법.\n",
    "\n",
    "#### 커널함수 의 종류\n",
    "\n",
    "고차원으로 직접 변환해서 내적을 하지않고도 내적값을 구할 수 있는 함수들이 이미 알려져 있고, 이를 커널트릭이라고 한다.\n",
    "Scikit-Learn의 [`SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) 클래스에서 제공하는 커널의 종류는 다음과 같다. 각 변수에 대한 자세한 내용은 해당 링크를 참고하면 된다. \n",
    "\n",
    "- **선형(linear)** : $K(\\mathbf{a}, \\mathbf{b}) = \\mathbf{a}^{T} \\cdot \\mathbf{b}$\n",
    "- **다항식(poly)** : $K(\\mathbf{a}, \\mathbf{b}) = \\left( \\gamma \\mathbf{a}^{T} \\cdot  \\mathbf{b} + r \\right)^{d}$\n",
    "- **가우시안 RBF(rbf)** : $K(\\mathbf{a}, \\mathbf{b}) = \\text{exp} \\left( -\\gamma \\| \\mathbf{a} - \\mathbf{b} \\|^{2} \\right)$\n",
    "- **시그모이드(sigmoid)** : $K(\\mathbf{a}, \\mathbf{b}) = \\tanh {\\left( \\gamma \\mathbf{a}^{T} \\cdot  \\mathbf{b} + r \\right)}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
