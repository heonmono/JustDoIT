{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‘Š CHAPTER 15 Autoencoder(ì˜¤í† ì¸ì½”ë”)\n",
    "\n",
    "#### ğŸ™‹ Chapter Manager : ì¥ìŠ¹ì¤€  ğŸ™ Contents Add :   ë‚¨ì°½í—Œ             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Autoencoder    \n",
    "              \n",
    "### &nbsp;&nbsp;ğ ì •ì˜    \n",
    "\n",
    "   - label ë˜ì–´ ìˆì§€ ì•Šì€ Train Dataë¥¼ ì‚¬ìš©í•˜ì—¬ (Unsupervised Learning) ì…ë ¥ dataì˜ íš¨ìœ¨ì ì¸ í‘œí˜„ì¸    \n",
    "     codingì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ANN(Artificial Neural Network)\n",
    "          \n",
    "     â€» coding : ì»´í“¨í„° í”„ë¡œê·¸ë˜ë°ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ë¶€í˜¸í™”ë¥¼ ì˜ë¯¸ (ì£¼ì–´ì§„ ì •ë³´ë¥¼ ì–´ë–¤ í‘œì¤€ì ì¸ í˜•íƒœë¡œ ë³€í™˜í•˜ê±°ë‚˜ ê±°ê¾¸ë¡œ ë³€í™˜í•¨.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ íŠ¹ì§•    \n",
    "\n",
    "   - ì¼ë°˜ì ìœ¼ë¡œ ì…ë ¥ë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ì°¨ì›ì„ ê°€ì§€ë¯€ë¡œ autoencoderê°€ ì°¨ì› ì¶•ì†Œì— ìœ ìš©     \n",
    "   - ê°•ë ¥í•œ íŠ¹ì„± ì¶”ì¶œê¸°ì²˜ëŸ¼ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— ì‹¬ì¸µ ì‹ ê²½ë§(DNN)ì˜ unspervised learning prior train ì‚¬ìš© ê°€ëŠ¥    \n",
    "   - Train dataì™€ ë§¤ìš° ë¹„ìŠ·í•œ ìƒˆë¡œìš´ dataë¥¼ ìƒì„± ê°€ëŠ¥ => Generative Modelì´ë¼ ì¹­í•¨    \n",
    "     ex. ì–¼êµ´ ì‚¬ì§„ì„ ì‚¬ìš©í•´ autoencoderë¥¼ í›ˆë ¨ì‹œí‚¤ë©´ ì´ ëª¨ë¸ì€ ìƒˆë¡œìš´ ì–¼êµ´ì„ ìƒì„± ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ ì›ë¦¬\n",
    "   - Autoencoderê°€ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë‹¨ìˆœíˆ ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒ\n",
    "   - Networkì— ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ì œì•½ì„ ê°€í•´ ì˜¤íˆë ¤ ì–´ë ¤ìš´ ì‘ì—…ìœ¼ë¡œ ë§Œë“¦  \n",
    "     ex. ë‚´ë¶€í‘œí˜„ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ê±°ë‚˜ ì…ë ¥ì— ì¡ìŒì„ ì¶”ê°€í•œ í›„ ì›ë³¸ ì…ë ¥ì„ ë³µì›í•  ìˆ˜ ìˆë„ë¡ networkë¥¼ í›ˆë ¨ì‹œí‚´    \n",
    "            \n",
    "     â€» ë‚´ë¶€ í‘œí˜„ì˜í¬ê¸°ë¥¼ ì œí•œ : hidden layerì— ìˆëŠ” neural ìˆ˜ë¥¼ input layerë³´ë‹¤ ì ê²Œí•œë‹¤ëŠ” ëœ»\n",
    "        \n",
    "1. ì…ë ¥ì„ ë°›ì•„ íš¨ìœ¨ì ì¸ ë‚´ë¶€ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê³  ì…ë ¥ê³¼ ë§¤ìš° ê°€ê¹Œì›Œ ë³´ì´ëŠ” ì–´ë–¤ ê²ƒì„ ì¶œë ¥ output layerì˜ neural ìˆ˜ê°€   \n",
    "   input layerì™€ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì œì™¸í•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ autoencoderëŠ” MLP(Multi Layer Perceptron)ê³¼ ë™ì¼í•œ êµ¬ì¡°                 \n",
    "         \n",
    "            \n",
    "2. autoencoderê°€ ì…ë ¥ì„ ì¬êµ¬ì„±í•˜ê¸° ë•Œë¬¸ì— ì¶œë ¥ì„ reconstruction(ì¬êµ¬ì„±)ì´ë¼ê³  ë¶€ë¦„  \n",
    "   cost functionì€ ì¬êµ¬ì„±ì´ ì…ë ¥ê³¼ ë‹¤ë¥¼ ë•Œ modelì— ë²Œì ì„ ë¶€ê³¼í•˜ëŠ” reconstruction loss(ì¬êµ¬ì„± ì†ì‹¤)ì„ í¬í•¨    \n",
    "       \n",
    "         \n",
    "3. ë‚´ë¶€ í‘œí˜„ì´ ì…ë ¥ dataë³´ë‹¤ ì €ì°¨ì›ì´ê¸° ë•Œë¬¸ì— ì´ëŸ° autoencoderë¥¼ Undercomplete(ê³¼ì†Œì™„ì „)ë¼ í•¨    \n",
    "   Undercomplete autoencoderëŠ” ì…ë ¥ì„ codingìœ¼ë¡œ ê°„ë‹¨íˆ ë³µì‚¬í•  ìˆ˜ ì—†ìœ¼ë©°, ì…ë ¥ê³¼ ë˜‘ê°™ì€ ê²ƒì„ ì¶œë ¥í•˜ê¸° ìœ„í•œ ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•„ì•¼í•˜ë©°,   \n",
    "   ì´ëŠ” ì…ë ¥ dataì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ì„ í•™ìŠµí•˜ë„ë¡ ë§Œë“¦(ê·¸ë¦¬ê³  ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²ƒì€ ë²„ë¦¼)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ êµ¬ì„±\n",
    "- encoderì™€ decoderë¡œ êµ¬ì„±\n",
    "- encoder : ì…ë ¥ì„ ë‚´ë¶€ë¡œ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì—­í•  (recognition network, ì¸ì§€ ë„¤íŠ¸ì›Œí¬)\n",
    "- decoder : ë‚´ë¶€ í‘œí˜„ì„ ì¶œë ¥ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì—­í•  ( generative network, ìƒì„± ë„¤íŠ¸ì›Œí¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Undercomplete(ê³¼ì†Œì™„ì „) ì„ í˜• autoencoderë¡œ PCA ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autoencoderê°€ ì„ í˜• í™œì„±í™” í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ê³  cost functionì´ MSE(í‰ê·  ì œê³± ì˜¤ì°¨)ë¼ë©´ ì´ëŠ” ê²°êµ­ PCAë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ    \n",
    "  PCAëŠ” ë¶„ì‚°ì´ ê°€ì¥ í° ë°©í–¥(ì£¼ì„±ë¶„, principal component)ë¥¼ ì°¾ìŒ (ì›ë³¸ ë°ì´í„°ì™€ íˆ¬ì˜ ê±°ë¦¬(euclidean distance)ë¥¼ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ” ì„±ë¶„ì„ ì°¾ëŠ” ê²ƒ)    \n",
    "        \n",
    "  ë”°ë¼ì„œ íˆ¬ì˜ ê±°ë¦¬ë¥¼ ê°€ì¥ ì‘ê²Œ ë§Œë“œëŠ” ê²ƒì€ MSEë¥¼ ìµœì†Œí™”í•˜ë ¤ëŠ” autoencoderì˜ ëª©ì ê³¼ ë™ì¼    \n",
    "  But PCAë¡œ ì°¾ì€ ì£¼ì„±ë¶„ê³¼ autoencoderì˜ codingì€ ê°™ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-3ca701fb2a5e>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\jjun1\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as rnd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "n_inputs = 3 # 3D ì…ë ¥\n",
    "n_hidden = 2 # 2D ì…ë ¥\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "hidden = tf.layers.dense(X, n_hidden)\n",
    "outputs = tf.layers.dense(hidden, n_outputs)\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì¶œë ¥ì˜ ê°œìˆ˜ê°€ ì…ë ¥ì˜ ê°œìˆ˜ì™€ ë™ì¼\n",
    "- ë‹¨ìˆœí•œ PCAë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” í™œì„±í™” í•¨ìˆ˜(activation function)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©°( ì¦‰, ëª¨ë“  neuralì´ ì„ í˜•) ,cost functionì€ MSE.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2ccef6ddfe8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# ë°ì´í„° ì…‹ì„ ë¡œë“œ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m  \u001b[1;31m# codingì„ ë§Œë“œëŠ” hidden layerë¥¼ ì¶œë ¥\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "X_train, X_test = [...] # ë°ì´í„° ì…‹ì„ ë¡œë“œ\n",
    "\n",
    "n_iterations = 1000\n",
    "codings = hidden  # codingì„ ë§Œë“œëŠ” hidden layerë¥¼ ì¶œë ¥  \n",
    "\n",
    "with tf.Session() as sess :\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations) :\n",
    "        training_op.run( feed_dict = { X : X_train}) # labelì´ ì—†ìŒ( unspervised learning )\n",
    "    codings_val = codings.eval(feed_dict = {X : X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> ì›ë³¸ data setê³¼ autoencoderì˜ hidden layerì˜ ì¶œë ¥( ì¦‰, coding layer)  \n",
    "autoencoderëŠ” dataì— ìˆëŠ” ë¶„ì‚°ì´ ê°€ëŠ¥í•œ ë§ì´ ë³´ì¡´ë˜ë„ë¡ dataë¥¼ íˆ¬ì˜í•  ìµœìƒì˜ 2D í‰ë©´ì„ ì°¾ìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Stacked(ì ì¸µ) Autoencoder (Deep Autoencoder, ì‹¬ì¸µ ì˜¤í† ì¸ì½”ë”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename = \"Ch 15. Stacked Autoencoder êµ¬ì¡°.JPG\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ íŠ¹ì§•\n",
    "- Autoencoderë„ ì—¬ëŸ¬ ê°œì˜ hidden layerë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìœ¼ë©´ì„œ ë‚˜ì˜¨ Stacked Autoencoder   \n",
    "  layerë¥¼ ë” ì¶”ê°€í•˜ë©´ autoencoderê°€ ë” ë³µì¡í•œ codingì„ í•™ìŠµ ê°€ëŠ¥ (But autoencoderê°€ ë„ˆë¬´ ê°•ë ¥í•˜ê²Œ ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜)  \n",
    "    \n",
    "        \n",
    "- encoderê°€ ë„ˆë¬´ ê°•ë ¥í•´ì„œ ê°ê°ì˜ ì…ë ¥  dataë¥¼ ì„ì˜ì˜ í•œ ìˆ«ìë¡œ mappingí•˜ë„ë¡ í•™ìŠµ, decoderëŠ” ì—­ìœ¼ë¡œ mapping í•˜ëŠ” ê²ƒì„ í•™ìŠµí–ˆë‹¤ê³  í•˜ë©´  \n",
    "  í›ˆë ¨ dataëŠ” ì™„ë²½í•˜ê²Œ ì¬êµ¬ì„±ì„ í•˜ê² ì§€ë§Œ ìœ ìš©í•œ ë°ì´í„° í‘œí˜„ì„ í•™ìŠµí•˜ì§€ëŠ” ëª»í•  ê²ƒ   \n",
    "       \n",
    "          \n",
    "- Stacked Autoencoderì˜ êµ¬ì¡°ëŠ” ì „í˜•ì ìœ¼ë¡œ hidden layer(coding layer)ì„ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­! (ìƒŒë“œìœ„ì¹˜ ëª¨ì–‘)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Stacked AutoencoderëŠ” í‰ë²”í•œ ì‹¬ì¸µ MLPê³¼ ë§¤ìš° ë¹„ìŠ·í•˜ê²Œ êµ¬í˜„ ê°€ëŠ¥    \n",
    "   - He ì´ˆê¸°í™”, ELU í™œì„±í™” í•¨ìˆ˜, $l$2 ê·œì œë¥¼ ì‚¬ìš©í•´ MNISTë¥¼ ìœ„í•œ Stacked Encoder (labelì´ ì—†ëŠ” ê²ƒì„ ì œì™¸í•˜ë©´ ì• chapterì—ì„œ ë‹¤ë£¬ ì½”ë“œë“¤ê³¼ ìœ ì‚¬)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28 #MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer() # He initialization\n",
    "#Equivalent to:\n",
    "#he_init = lambda shape, dtype=tf.float32: tf.truncated_normal(shape, 0., stddev=np.sqrt(2/shape[0]))\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "my_dense_layer = partial(tf.layers.dense,\n",
    "                         activation=tf.nn.elu,\n",
    "                         kernel_initializer=he_init,\n",
    "                         kernel_regularizer=l2_regularizer)\n",
    "\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3 = my_dense_layer(hidden2, n_hidden3)\n",
    "outputs = my_dense_layer(hidden3, n_outputs, activation=None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() # not shown in the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):  # batch_size\n",
    "        n_batches = mnist.train.num_examples // batch_size   \n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\") # not shown in the book\n",
    "            sys.stdout.flush()                                          # not shown\n",
    "            # shuffle_batch() í•¨ìˆ˜ì •ì˜ëŠ” githubë¥¼ ì°¸ê³ \n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})   # not shown\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)           # not shown\n",
    "        saver.save(sess, \"./my_model_all_layers.ckpt\")                  # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructed_digits(X, outputs, model_path = None, n_test_digits = 2):\n",
    "    with tf.Session() as sess:\n",
    "        if model_path:\n",
    "            saver.restore(sess, model_path)\n",
    "        X_test = mnist.test.images[:n_test_digits]\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_test})\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 3 * n_test_digits))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(X_test[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"autoencoders\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    #path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(fig_id + \".png\", format='png', dpi=300)\n",
    "\n",
    "def plot_image(image, shape=[28, 28]):\n",
    "    plt.imshow(image.reshape(shape), cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "show_reconstructed_digits(X, outputs, \"./my_model_all_layers.ckpt\")\n",
    "save_fig(\"reconstruction_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ Weight(ê°€ì¤‘ì¹˜) ë¬¶ê¸°\n",
    "- Stacked Autoencoderê°€ ì™„ë²½í•˜ê²Œ ëŒ€ì¹­ì¼ ë•Œ(ìƒŒë“œìœ„ì¹˜ í˜•íƒœ) ì¼ë°˜ì ìœ¼ë¡œ decoderì˜ ê°€ì¤‘ì¹˜ì™€ encoderì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¬¶ìŒ  \n",
    "- Decoderì™€ Encoderì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¬¶ìœ¼ë©´ Modelì— ìˆëŠ” ê°€ì¤‘ì¹˜ì˜ ê°¯ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—¬ì„œ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì´ê³  overfittingì˜ ìœ„í—˜ì„ ì¤„ì—¬ì¤Œ   \n",
    "    \n",
    "       \n",
    "- ì„ì˜ì˜ Autoencoderê°€ (input layerì€ ì œì™¸í•œ) Nê°œì˜ ì¸µì„ ê°€ì§€ê³  ìˆê³  $W_{L}$ì´ Lë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  í–ˆì„ ë•Œ ì˜ˆë¥¼ ë“¤ì–´ 1ì€ ì²« ë²ˆì§¸ hidden layer, $ \\frac{N}{2}$ì€ coding layer, $N$ì€ output layer)     \n",
    "    \n",
    "       \n",
    "- Decoder layerì˜ ê°€ì¤‘ì¹˜ : $W_{N-L+1} = W_{L}^{T}$ ($L = 1,2, ... , \\frac{N}{2}$)  \n",
    "  (weights_decoder = tf.transpose(weights_encoder))\n",
    "- tensorflowì—ì„œ dense() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ë¬¶ìœ¼ë ¤ë©´ ì¡°ê¸ˆ ë³µì¡í•˜ê¸°ì— ì§ì ‘ layerë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì´ ë” ì‰¬ì›€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.transpose(weights2, name=\"weights3\")  # tied weights\n",
    "weights4 = tf.transpose(weights1, name=\"weights4\")  # tied weights\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2)\n",
    "loss = reconstruction_loss + reg_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- weights3ì™€ weights4ëŠ” ë³€ìˆ˜ë¡œ ì„ ì–¸ë˜ì§€ ì•Šì•˜ê³  ê°ê° weights2ì™€ weights1ì˜ ì „ì¹˜ì…ë‹ˆë‹¤.(ì„œë¡œ ë¬¶ì—¬ìˆìŒ)\n",
    "- ë³€ìˆ˜ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ê·œì œì— ì‚¬ìš©ë˜ì§€ ì•ŠìŒ. weights1ê³¼ weightsë§Œ ê·œì œ\n",
    "- í¸í–¥ì€ ë¬¶ì§€ë„ ì•Šê³  ê·œì œí•˜ì§€ë„ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        saver.save(sess, \"./my_model_tying_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_reconstructed_digits(X, outputs, \"./my_model_tying_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ í•œ ë²ˆì— í•œ ì¸µì”© í›ˆë ¨\n",
    "ì „ì²´ autoencoderë¥¼ í›ˆë ¨ ì‹œí‚¤ê¸°ë³´ë‹¨ í•œ ë²ˆì— autoencoder í•˜ë‚˜ë¥¼ í›ˆë ¨í•˜ê³  ì´ë¥¼ ìŒ“ì•„ì˜¬ë ¤ í•œ ê°œì˜ Stacked Autoencoder í•˜ë‚˜ë¥¼ í›ˆë ¨í•˜ê³  ì´ë¥¼ ìŒ“ì•„ì˜¬ë ¤ í•œ ê°œì˜ Stacked Autoencoderë¥¼ ë§Œë“œëŠ” ê²ƒì´ ë” ë¹ ë¦„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Ch 15. í•œ ë²ˆì— autoencoder í•œ ê°œì”© í›ˆë ¨í•˜ê¸°.jpg\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì´ëŸ¬í•œ ë‹¤ë‹¨ê³„ í›ˆë ¨ Algorithmì„ êµ¬í˜„í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€   \n",
    "              \n",
    " 1. í•˜ë‚˜ì˜ autoencoder í›ˆë ¨ì´ ëë‚˜ë©´ í›ˆë ¨ setì„ networkì— í†µê³¼ì‹œì¼œ hidden layerì˜ ì¶œë ¥ì„ ì €ì¥\n",
    " 2. 1.ì—ì„œì˜ ì¶œë ¥ì€ ë‹¤ìŒ autoencoderì˜ í›ˆë ¨ setì´ ë¨\n",
    " 3. ëª¨ë“  autoencoderê°€ ì´ëŸ° ì‹ìœ¼ë¡œ í›ˆë ¨ì„ ë§ˆì¹˜ë©°(1ê³¼ 2ë¥¼ ë°˜ë³µ) ê° autoencoderì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë³µì‚¬í•´ì„œ stacked autoencoderë¥¼ ë§Œë“¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                      learning_rate = 0.01, l2_reg = 0.0005, seed=42,\n",
    "                      hidden_activation=tf.nn.elu,\n",
    "                      output_activation=tf.nn.elu):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, activation=hidden_activation, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=output_activation, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train two Autoencoders. The first one is trained on the training data, and the second is trained on the previous Autoencoder's hidden layer output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_output, W1, b1, W4, b4 = train_autoencoder(mnist.train.images, n_neurons=300, n_epochs=4, batch_size=150,\n",
    "                                                  output_activation=None)\n",
    "_, W2, b2, W3, b3 = train_autoencoder(hidden_output, n_neurons=150, n_epochs=4, batch_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a Stacked Autoencoder by simply reusing the weights and biases from the Autoencoders we just trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)\n",
    "hidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)\n",
    "hidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)\n",
    "outputs = tf.matmul(hidden3, W4) + b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructed_digits(X, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ Stacked Autoencoderë¥¼ í›ˆë ¨í•˜ëŠ” í•˜ë‚˜ì˜ Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Ch 15. Stacked Autoencoderë¥¼ í›ˆë ¨í•˜ëŠ” í•˜ë‚˜ì˜ graph.JPG\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ê·¸ë˜í”„ ì¤‘ì•™ ë¶€ë¶„ì€ ì „ì²´ Stacked Autoencoder (í›ˆë ¨ì´ ëë‚œ ë’¤ì— ì‚¬ìš©)\n",
    "2. ê·¸ë˜í”„ ì™¼ìª½ë¶€ë¶„ì€ í›ˆë ¨ ë‹¨ê³„ 1ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì—°ì‚°. \n",
    "3. ì—¬ê¸°ì—ì„  hidden layer 2ì™€ 3ì„ ê·¸ëƒ¥ ì§€ë‚˜ì¹˜ëŠ” output layerì„ ë§Œë“¦. -> ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ stacked autoencoderì˜ output layerì™€ ê³µìœ \n",
    "4. 3.ìœ„ì— ì¶œë ¥ì„ ê°€ëŠ¥í•œ í•œ ì…ë ¥ê³¼ ê°€ê¹ê²Œ ë§Œë“¤ê¸° ìœ„í•œ í›ˆë ¨ ì—°ì‚°ì´ ìˆìœ¼ë¯€ë¡œ 'hidden layer 1'ê³¼ 'output layer 2'ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ì‹œí‚´   \n",
    "   ( ì¦‰, ì²« ë²ˆì§¸ì˜ autoencoder )   \n",
    "5. ê·¸ë˜í”„ì˜ ì˜¤ë¥¸ìª½ ë¶€ë¶„ì€ í›ˆë ¨ ë‹¨ê³„ 2ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì—°ì‚°. \n",
    "6. hidden layer 3ì˜ ì¶œë ¥ì„ hidden layer 1ì˜ ì¶œë ¥ê³¼ ê°€ëŠ¥í•œ í•œ ê°€ê¹ê²Œ ë§Œë“¤ê¸° ìœ„í•œ í›ˆë ¨ ì—°ì‚°ì´ ì¶”ê°€ë¨. \n",
    "7. ë‹¨ê³„ 2ë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆì—ëŠ” hidden layer 1ì„ ë™ê²°í•´ì•¼ í•¨ ->  hidden layer 2ì™€ hidden layer 3ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ì‹œí‚´    \n",
    "   ( ì¦‰, ë‘ ë²ˆì§¸ì˜ autoencoder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0001\n",
    "\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights3_init = initializer([n_hidden2, n_hidden3])\n",
    "weights4_init = initializer([n_hidden3, n_outputs])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.Variable(weights3_init, dtype=tf.float32, name=\"weights3\")\n",
    "weights4 = tf.Variable(weights4_init, dtype=tf.float32, name=\"weights4\")\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name=\"biases3\")\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name=\"biases4\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "outputs = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "with tf.name_scope(\"phase1\"):\n",
    "    phase1_outputs = tf.matmul(hidden1, weights4) + biases4  # bypass hidden2 and hidden3\n",
    "    phase1_reconstruction_loss = tf.reduce_mean(tf.square(phase1_outputs - X))\n",
    "    phase1_reg_loss = regularizer(weights1) + regularizer(weights4)\n",
    "    phase1_loss = phase1_reconstruction_loss + phase1_reg_loss\n",
    "    phase1_training_op = optimizer.minimize(phase1_loss)\n",
    "\n",
    "with tf.name_scope(\"phase2\"):\n",
    "    phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1))\n",
    "    phase2_reg_loss = regularizer(weights2) + regularizer(weights3)\n",
    "    phase2_loss = phase2_reconstruction_loss + phase2_reg_loss\n",
    "    train_vars = [weights2, biases2, weights3, biases3]\n",
    "    phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars) # freeze hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ops = [phase1_training_op, phase2_training_op]\n",
    "reconstruction_losses = [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
    "n_epochs = [4, 4]\n",
    "batch_sizes = [150, 150]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for phase in range(2):\n",
    "        print(\"Training phase #{}\".format(phase + 1))\n",
    "        for epoch in range(n_epochs[phase]):\n",
    "            n_batches = mnist.train.num_examples // batch_sizes[phase]\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase])\n",
    "                sess.run(training_ops[phase], feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_losses[phase].eval(feed_dict={X: X_batch})\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "            saver.save(sess, \"./my_model_one_at_a_time.ckpt\")\n",
    "    loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
    "    print(\"Test MSE:\", loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ë‹¨ê³„1ì€ hidden layer 2ì™€ 3ì„ ê±´ë„ˆë›°ëŠ” output layerë¥¼ ë§Œë“¤ê³ , ì´ ì¶œë ¥ê³¼ ì…ë ¥ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ (+ ì¼ë¶€ ê·œì œ) ìµœì†Œí™”í•˜ëŠ” í›ˆë ¨ ì—°ì‚°ì„ ë§Œë“¦\n",
    "    \n",
    "      \n",
    "2. ë‹¨ê³„2ëŠ” hidden layer3ê³¼ hidden layer1 ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼(+ ì¼ë¶€ ê·œì œ) ìµœì†Œí™”í•˜ëŠ”ë° í•„ìš”í•œ ì—°ì‚° ì¶”ê°€   \n",
    "ê°€ì¥ ì¤‘ìš”í•œ ì ì€ minimize() methodì— í›ˆë ¨ë  ë³€ìˆ˜ ëª©ë¡ì„ ë„£ëŠ”ë° weights1ê³¼ biases1ì€ ì œì™¸ (=> hidden layer 1ì„ ë™ê²°ì‹œí‚¤ëŠ” íš¨ìœ¨ì ì¸ ë°©ë²•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total\n",
    "  1. ë‹¨ê³„ 1ì˜ í›ˆë ¨ ì—°ì‚°ì— ëŒ€í•´ ì—¬ëŸ¬ ë²ˆ epochë¥¼ ì‹¤í–‰\n",
    "  2. ë‹¨ê³„ 2ì˜ í›ˆë ¨ ì—°ì‚°ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ë§ì€ epochë¥¼ ì‹¤í–‰\n",
    "\n",
    "- Tip\n",
    "  1. hidden layer1ì´ ë‹¨ê³„ 2ì—ì„œ ë™ê²°ë˜ê¸° ë•Œë¬¸ì— ê° í›ˆë ¨ sampleì— ëŒ€í•œ hidden layerì˜ ì¶œë ¥ì€ ë™ì¼.\n",
    "  2. hidden layer 1ì˜ ì¶œë ¥ì„ ë§¤ epochë§ˆë‹¤ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•Šë„ë¡ í•˜ë ¤ë©´ ë‹¨ê³„ 1ì˜ ëì—ì„œ ì „ì²´ í›ˆë ¨ setì— ëŒ€í•´ hidden layer 1ì˜ ì¶œë ¥ì„ ê³„ì‚°\n",
    "  3. ë‹¨ê³„ 2ë™ì•ˆ hidden layer 2ì˜ ìºì‹±ëœ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ì£¼ì…  \n",
    "### => í›ˆë ¨ ì†ë„ë¥¼ ìƒë‹¹íˆ ë†’ì¼ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ Cache the frozen layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_ops = [phase1_training_op, phase2_training_op]\n",
    "reconstruction_losses = [phase1_reconstruction_loss, phase2_reconstruction_loss]\n",
    "n_epochs = [4, 4]\n",
    "batch_sizes = [150, 150]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for phase in range(2):\n",
    "        print(\"Training phase #{}\".format(phase + 1))\n",
    "        if phase == 1:\n",
    "            hidden1_cache = hidden1.eval(feed_dict={X: mnist.train.images})\n",
    "        for epoch in range(n_epochs[phase]):\n",
    "            n_batches = mnist.train.num_examples // batch_sizes[phase]\n",
    "            for iteration in range(n_batches):\n",
    "                print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                if phase == 1:\n",
    "                    indices = rnd.permutation(mnist.train.num_examples)\n",
    "                    hidden1_batch = hidden1_cache[indices[:batch_sizes[phase]]]\n",
    "                    feed_dict = {hidden1: hidden1_batch}\n",
    "                    sess.run(training_ops[phase], feed_dict=feed_dict)\n",
    "                else:\n",
    "                    X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase])\n",
    "                    feed_dict = {X: X_batch}\n",
    "                    sess.run(training_ops[phase], feed_dict=feed_dict)\n",
    "            loss_train = reconstruction_losses[phase].eval(feed_dict=feed_dict)\n",
    "            print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "            saver.save(sess, \"./my_model_cache_frozen.ckpt\")\n",
    "    loss_test = reconstruction_loss.eval(feed_dict={X: mnist.test.images})\n",
    "    print(\"Test MSE:\", loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ Visualizing the Reconstructions\n",
    "- Autoencoderê°€ ì œëŒ€ë¡œ í›ˆë ¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë²•\n",
    "  1. ì…ë ¥ê³¼ ì¶œë ¥ì„ ë¹„êµ\n",
    "    - ì…ë ¥ê³¼ ì¶œë ¥ì€ ë§¤ìš° ë¹„ìŠ·í•´ì•¼ í•˜ê³ , ì°¨ì´ê°€ ìˆë‹¤ë©´ ì¤‘ìš”í•œ ë¶€ë¶„ì´ ì•„ë‹ˆì—¬ì•¼ í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ë¡œ ì„ íƒí•œ ë‘ ê°œì˜ ìˆ«ìë¡œ ì´ìš©í•´ ì¬êµ¬ì„±ì„ í™•ì¸\n",
    "\n",
    "n_test_digits = 2\n",
    "X_test = mnist.test.images[:n_test_digits]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_one_at_a_time.ckpt\") # not shown in the book\n",
    "    outputs_val = outputs.eval(feed_dict = { X : X_test[:n_test_digits]})\n",
    "    \n",
    "def plot_image( image, shape = [28, 28]) :\n",
    "    plt.imshow(image.reshape(shape) , cmap = \"Greys\", interpolation = \"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "for digit_index in range(n_test_digits) :\n",
    "    plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "    plot_image(X_test[digit_index])\n",
    "    plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "    plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ Feature Visualization (Visualizing the extracted features)\n",
    "Autoencoderê°€ íŠ¹ì • íŠ¹ì„±ì„ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ ì´ íŠ¹ì • íŠ¹ì„±ì„ í™•ì¸í•´ë³´ê³  ì‹¶ì„ ê²ƒ\n",
    "1. ê°€ì¥ ê°„ë‹¨í•œ ê¸°ë²•ìœ¼ë¡œ hidden layerì— ìˆëŠ” ê° ë‰´ëŸ°ì„ ê°€ì¥ í¬ê²Œ í™œì„±í™”ì‹œí‚¤ëŠ” í›ˆë ¨ sampleì„ ì°¾ëŠ” ê²ƒ.   \n",
    "=> ìµœìƒë‹¨ì˜ hidden layerì— ìœ ìš©, ë¹„êµì  í° íŠ¹ì„±ë“¤ì„ ì¡ì•„ë‚´ê¸°ì— ì´ë¥¼ í¬í•¨í•œ sampleì„ ì°¾ê¸° ì‰¬ì›€    \n",
    "ex. í•œ ë‰´ëŸ°ì´ ê³ ì–‘ì´ê°€ ìˆëŠ” imageë¥¼ ë³´ì•˜ì„ ë•Œ ê°•í•˜ê²Œ í™œì„±í™”ê°€ ë˜ì—ˆë‹¤ë©´ ì´ ë‰´ëŸ°ì„ í™œì„±í™” ì‹œí‚¤ëŠ” imageì—ëŠ” ëŒ€ë¶€ë¶„ ê³ ì–‘ì´ê°€ ë“¤ì–´ìˆì„ ê²ƒ.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But í•˜ìœ„ì¸µì€ íŠ¹ì„±ì˜ í¬ê¸°ê°€ ì‘ê³  ì¶”ìƒì ì´ê¸°ì— ë‰´ëŸ°ì´ ë¬´ì—‡ë•Œë¬¸ì— í™œì„±í™”ê°€ ë˜ì—ˆëŠ”ì§€ ì´í•´í•˜ê¸° ì–´ë ¤ì›€ ë”°ë¼ì„œ, ì´ëŸ¬í•œ ê¸°ë²•ì´ ì í•©í•˜ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_one_at_a_time.ckpt\") # not shown in the book\n",
    "    weights1_val = weights1.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_image(weights1_val.T[i])\n",
    "\n",
    "save_fig(\"extracted_features_plot\") # not shown\n",
    "plt.show()                          # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Randomí•œ ì…ë ¥ imageë¥¼ autoencoderì— ì£¼ì…í•˜ê³ , ê´€ì‹¬ ìˆëŠ” neuronsì˜ í™œì„±í™”ë¥¼ ê´€ì°°í•´ì„œ ì´ neuronsì´ ë” í™œì„±í™”ë˜ëŠ” ë°©í–¥ìœ¼ë¡œ imageë¥¼ ìˆ˜ì •í•˜ë„ë¡ backpropagationë¥¼ ìˆ˜í–‰   \n",
    "-> ì´ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•˜ë©´(gradient ascent(ê²½ì‚¬ìƒìŠ¹ë²•)ë¥¼ ìˆ˜í–‰í•˜ì—¬) imageëŠ” ì ì°¨(ê·¸ neuronsì„ ìœ„í•´) ë†€ë¼ìš´ ë°©í–¥ìœ¼ë¡œ ë°”ë€Œê²Œ ë¨.   \n",
    "ì´ëŠ” neuronsì´ ë°”ë¼ë³´ëŠ” ì…ë ¥ì˜ ì¢…ë¥˜ë¥¼ ì‹œê°í™”í•˜ëŠ”ë° ìœ ìš©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Unsupervised pretraining (Stacked Autoencoderë¥¼ ì‚¬ìš©í•œ Unsupervised pretraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ ì ìš©    \n",
    "     \n",
    "1. label ë˜ì–´ ìˆëŠ” í›ˆë ¨ dataê°€ ë§ì§€ ì•Šì€ ë³µì¡í•œ supervised learning ë¬¸ì œë¥¼ ë‹¤ë¤„ì•¼í•œë‹¤ë©´,  \n",
    "   - ë¹„ìŠ·í•œ ë¬¸ì œì— í•™ìŠµëœ ì‹ ê²½ë§ì„ ì°¾ì•„ í•˜ìœ„ì¸µì„ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒ.  \n",
    "   => ì €ìˆ˜ì¤€ì˜ íŠ¹ì„±ì„ í•™ìŠµí•  í•„ìš”ê°€ ì—†ê¸°ì— ì ì€ í›ˆë ¨ dataë§Œ ì‚¬ìš©í•´ë„ ê³ ì„±ëŠ¥ model í›ˆë ¨ì´ ê°€ëŠ¥\n",
    "   ì¦‰, ê¸°ì¡´ì˜ networkì—ì„œ í•™ìŠµí•œ íŠ¹ì„± ê°ì§€ ê¸°ëŠ¥ì„ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒ\n",
    "  \n",
    "2. ëŒ€ë¶€ë¶„ label ë˜ì–´ ìˆì§€ ì•Šì€ ëŒ€ëŸ‰ì˜ data setì´ ìˆë‹¤ë©´,  \n",
    "   1. ë¨¼ì € ì „ì²´ dataë¥¼ ì‚¬ìš©í•´ Stacked Autoencoderë¥¼ í›ˆë ¨ ì‹œí‚¬ ìˆ˜ ìˆìŒ\n",
    "   2. Autoencoderì˜ í•˜ìœ„ì¸µì„ ì¬ì‚¬ìš©í•´ ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì‹ ê²½ë§ì„ ìœ„í•´ Unsupervised Learning Prior Trainì„ ìˆ˜í–‰í•˜ëŠ” Stack Autoencoderë¥¼ ì‚¬ìš©  \n",
    "   3. 535p ê·¸ë¦¼ 15-8ì€ ë¶„ë¥˜ì— ëŒ€í•œ neural networkì„ ìœ„í•´ unsupervised learning prior trainì„ ìˆ˜í–‰í•˜ëŠ” stacked autoencoderë¥¼ ì‚¬ìš©í•˜ëŠ” ë²•ì„ ë³´ì—¬ì¤Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoderë¥¼ ì‚¬ìš©í•œ Unspervised Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Ch 15. Autoencoderë¥¼ ì‚¬ìš©í•œ Unspervised pretrain.jpg\", width  = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ íŠ¹ì§•\n",
    "- Stacked AutoencoderëŠ” í•œ ë²ˆì— í•œ  layerì”© í›ˆë ¨ë˜ë¯€ë¡œ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨í•  ë•Œ,   \n",
    "  labelëœ í›ˆë ¨ dataê°€ ë§ì§€ ì•Šìœ¼ë©´ì„œ ì‚¬ì „í›ˆë ¨ëœ layerì„ ë™ê²°í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ  \n",
    "  (ì ì–´ë„ ê°€ì¥ í•˜ìœ„ì¸µ í•˜ë‚˜ëŠ”)\n",
    "\n",
    "- ëŒ€ëŸ‰ì˜ label ë˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ë§Œë“œëŠ” ë¹„ìš©ì´ ì ê²Œ ë“¤ê¸° ë•Œë¬¸ì— ì´ëŸ° ìƒí™©ì€ ì‹¤ì œë¡œ ìì£¼ ìˆìŒ   \n",
    "    \n",
    "      \n",
    "- ë°ì´í„°ë¥¼ labelí•˜ëŠ” ê²ƒì€ ì‚¬ëŒë§Œì´ ê°€ëŠ¥í•˜ê¸°ì— sampleì„ labelingí•˜ëŠ” ë°ëŠ” ì‹œê°„ê³¼ ë¹„ìš©ì´ ë§ì´ ì†Œëª¨ë˜ë¯€ë¡œ  \n",
    "  labelì´ ë˜ì§€ ì•Šì€ dataë³´ë‹¤ ì ì„ ì¼ì´ ì¦ìŒ\n",
    "\n",
    "- í¥ë¯¸ë¡œìš´ íŠ¹ì„±ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ autoencoderë¥¼ ê°•ì œí•˜ê¸° ìœ„í•´ coding layerì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ì—¬ undercompleteë¥¼ ë§Œë“¦ \n",
    "     \n",
    "       \n",
    "- ì‚¬ì‹¤ ë‹¤ë¥¸ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì œí•œì„ ì‚¬ìš© ê°€ëŠ¥    \n",
    "    \n",
    "      \n",
    "- coding layerì˜ í¬ê¸°ë¥¼ ì…ë ¥ layerì™€ ê°™ê²Œ í•˜ê±°ë‚˜ í˜¹ì€ ë” í¬ê²Œ í•˜ì—¬ overcomplete autoencoderë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŒ    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST ë¶„ë¥˜í•˜ëŠ” ì‘ì€ ì‹ ê²½ë§ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.0005\n",
    "\n",
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "weights1_init = initializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "weights3_init = initializer([n_hidden2, n_outputs])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype=tf.float32, name=\"weights1\")\n",
    "weights2 = tf.Variable(weights2_init, dtype=tf.float32, name=\"weights2\")\n",
    "weights3 = tf.Variable(weights3_init, dtype=tf.float32, name=\"weights3\")\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name=\"biases1\")\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name=\"biases2\")\n",
    "biases3 = tf.Variable(tf.zeros(n_outputs), name=\"biases3\")\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "logits = tf.matmul(hidden2, weights3) + biases3\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "reg_loss = regularizer(weights1) + regularizer(weights2) + regularizer(weights3)\n",
    "loss = cross_entropy + reg_loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "pretrain_saver = tf.train.Saver([weights1, weights2, biases1, biases2])\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular training (without pretraining):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "batch_size = 150\n",
    "n_labeled_instances = 20000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n_labeled_instances // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            indices = rnd.permutation(n_labeled_instances)[:batch_size]\n",
    "            X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train accuracy:\", accuracy_val, end=\" \")\n",
    "        saver.save(sess, \"./my_model_supervised.ckpt\")\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì²˜ìŒì— ì‚¬ìš©í•œ ë‘ ê°œì˜ autoencoder layerë¥¼ ì¬ì‚¬ìš© (reusing the first two layers of the autoencoder we pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "batch_size = 150\n",
    "n_labeled_instances = 20000\n",
    "\n",
    "training_op = optimizer.minimize(loss, var_list=[weights3, biases3])  # Freeze layers 1 and 2 (optional)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    pretrain_saver.restore(sess, \"./my_model_cache_frozen.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = n_labeled_instances // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            indices = rnd.permutation(n_labeled_instances)[:batch_size]\n",
    "            X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train accuracy:\", accuracy_val, end=\"\\t\")\n",
    "        saver.save(sess, \"./my_model_supervised_pretrained.ckpt\")\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Test accuracy:\", accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Stacked denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n",
    "* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n",
    "* the `is_training` parameter is renamed to `training`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autoencoderê°€ ìœ ìš©í•œ íŠ¹ì„±ì„ í•™ìŠµí•˜ë„ë¡ ê°•ì œí•˜ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ \n",
    "  1. ì…ë ¥ì— ì¡ìŒì„ ì¶”ê°€\n",
    "  2. noiseê°€ ì—†ëŠ” ì›ë³¸ ì…ë ¥ì„ ë³µì›í•˜ë„ë¡ í›ˆë ¨ ì‹œí‚¤ëŠ” ê²ƒ   \n",
    "     \n",
    "         \n",
    "           \n",
    "- autoencoderê°€ ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ë³µì‚¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ dataì— ìˆëŠ” patternì„ ì°¾ì•„ì•¼ í•¨\n",
    "\n",
    "  Stacked Denoising Autoencoder (ì ì¸µ ì¡ìŒì œê±° ì˜¤í† ì¸ì½”ë”)\n",
    "\n",
    "  noiseëŠ” ì…ë ¥ì— ì¶”ê°€ëœ ìˆœìˆ˜í•œ Gaussian(ê°€ìš°ì‹œì•ˆ) noiseì´ê±°ë‚˜ Drop outì²˜ëŸ¼ ë¬´ì‘ìœ„ë¡œ ì…ë ¥ì„ êº¼ì„œ ë°œìƒ ì‹œí‚¬ ìˆ˜ë„ ìˆìŒ   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Gaussian noise(left) & Dropout(right)ì„ ì‚¬ìš©í•œ Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Ch 15. Gaussian noise(left) & Dropout(right)ì„ ì‚¬ìš©í•œ Denoising Autoencoder.jpg\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 1.0\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_noisy = X + noise_level * tf.random_normal(tf.shape(X))\n",
    "\n",
    "hidden1 = tf.layers.dense(X_noisy, n_hidden1, activation=tf.nn.relu,\n",
    "                          name=\"hidden1\")\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, # not shown in the book\n",
    "                          name=\"hidden2\")                            # not shown\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, # not shown\n",
    "                          name=\"hidden3\")                            # not shown\n",
    "outputs = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")        # not shown\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        saver.save(sess, \"./my_model_stacked_denoising_gaussian.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 150  # codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                          name=\"hidden1\")\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, # not shown in the book\n",
    "                          name=\"hidden2\")                            # not shown\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, # not shown\n",
    "                          name=\"hidden3\")                            # not shown\n",
    "outputs = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")        # not shown\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, training: True})\n",
    "        loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        saver.save(sess, \"./my_model_stacked_denoising_dropout.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructed_digits(X, outputs, \"./my_model_stacked_denoising_dropout.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Sparse Autoencoder (í¬ì†Œ Autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì¢‹ì€ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ë„ë¡ ë§Œë“œëŠ” ë‹¤ë¥¸ ì œì•½ì˜ ë°©ì‹ì¸ Sparsity(í¬ì†Œ)   \n",
    "      \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;ğ ì›ë¦¬   \n",
    "   \n",
    "- cost functionì— ì ì ˆí•œ í•­ì„ ì¶”ê°€í•˜ì—¬ autoencoderê°€ coding layerì—ì„œ í™œì„±í™”ë˜ëŠ” ë‰´ëŸ° ìˆ˜ë¥¼ ê°ì†Œì‹œí‚¤ë„ë¡ ë§Œë“¦    \n",
    "    \n",
    "  => coding layerì—ì„œ í‰ê· ì ìœ¼ë¡œ 5% neuronsë§Œ ë‘ë“œëŸ¬ì§€ê²Œ í™œì„±í™”ë˜ë„ë¡ ê°•ì œ ê°€ëŠ¥    \n",
    "       \n",
    "  => autoencoderê°€ ì ì€ ìˆ˜ì˜ í™œì„±í™”ëœ ë‰´ëŸ°ì„ ì¡°í•©í•˜ì—¬ ì…ë ¥ì„ í‘œí˜„í•´ì•¼ í•¨     \n",
    "                 \n",
    "  coding layerì˜ ê° neuronì€ ìœ ìš©í•œ íŠ¹ì„±ì„ í‘œí˜„í•˜ê²Œ ë¨.(ìˆ˜ì…ì´ ë§¤ìš° ì ìœ¼ë©´ ì •ë§ í•„ìš”í•œ ê³³ì—ë§Œ ëˆì„ ì‚¬ìš©í•˜ê²Œë˜ëŠ” ê²ƒì²˜ëŸ¼)\n",
    "  \n",
    "  \n",
    "- Sparsity Modelì„ ë§Œë“¤ê¸° ìœ„í•´ ë¨¼ì € ê° í›ˆë ¨ ë°˜ë³µì—ì„œ coding layerì˜ ì‹¤ì œ saprsity ì •ë„ë¥¼ ì¸¡ì •í•´ì•¼ í•¨  \n",
    "  \n",
    "      \n",
    "- ì´ë¥¼ ìœ„í•´ ì „ì²´ í›ˆë ¨ ë°°ì¹˜ì— ëŒ€í•´ coding layerì˜ í‰ê· ì ì¸ í™œì„±í™”ë¥¼ ê³„ì‚°  \n",
    "      \n",
    "         \n",
    "- ë°°ì¹˜ í¬ê¸°ëŠ” ë„ˆë¬´ ì‘ì§€ ì•Šì•„ì•¼í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ í‰ê· ê°’ì´ ì •í™•í•˜ì§€ ì•ŠìŒ\n",
    "     \n",
    "     \n",
    "- ê° neuronsì— ëŒ€í•œ í‰ê·  í™œì„±í™” ì •ë„ë¥¼ ì•Œë©´ cost functionì— sparsity lossë¥¼ ì¶”ê°€í•˜ì—¬ neuronsì´ ë„ˆë¬´ í™œì„±í™”ë˜ì§€ ì•Šë„ë¡ ê·œì œí•  ìˆ˜ ìˆìŒ.  \n",
    "  ex. í•œ neuronsì˜ í‰ê·  í™œì„±í™”ê°€ 0.3ì´ê³  ëª©í‘œ sparsity ì •ë„ê°€ 0.1ì´ë¼ë©´ ì´ neuronsì€ ëœ í™œì„±í™”ë˜ë„ë¡ ê·œì œë˜ì–´ì•¼ í•¨.   \n",
    "     \n",
    "      \n",
    "- ê°„ë‹¨í•œ ë°©ë²•ì€ \n",
    "  1. ë¹„ìš© í•¨ìˆ˜ì— ì œê³± ì˜¤ì°¨ $(0.3 - 0.1)^{2}$ì„ ì¶”ê°€í•˜ëŠ” ê²ƒ\n",
    "ì‹¤ì „ì—ì„œ ë” ì¢‹ì€ ë°©ë²•ì€\n",
    "  2. ì•„ë˜ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ Kullback-Leibler Divergence(ì¿¨ë°± ë¼ì´ë¸”ëŸ¬ ë°œì‚°)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.1\n",
    "q = np.linspace(0.001, 0.999, 500)\n",
    "kl_div = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n",
    "mse = (p - q)**2\n",
    "plt.plot([p, p], [0, 0.3], \"k:\")\n",
    "plt.text(0.05, 0.32, \"Target\\nsparsity\", fontsize=14)\n",
    "plt.plot(q, kl_div, \"b-\", label=\"KL divergence\")\n",
    "plt.plot(q, mse, \"r--\", label=\"MSE\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Actual sparsity\")\n",
    "plt.ylabel(\"Cost\", rotation=0)\n",
    "plt.axis([0, 1, 0, 0.95])\n",
    "save_fig(\"sparsity_loss_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Kullback-Leibler Divergence (KL ë°œì‚°) \n",
    "\n",
    "$ D_{KL}(P||Q) = \\sum_i{P(i)log\\frac{P(i)}{Q(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "codingì—ì„œ neuronsì´ í™œì„±í™”ë  ëª©í‘œ í™•ë¥  pì™€ ì‹¤ì œ í™•ë¥  q(ì¦‰, í›ˆë ¨ ë°°ì¹˜ì— ëŒ€í•œ í‰ê·  í™œì„±í™”) ì‚¬ì´ì˜ ë°œì‚°ì„ ì¸¡ì •  \n",
    "ë”°ë¼ì„œ KL DivergenceëŠ” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ D_{KL} (p||q) = plog\\frac{p}{q} + (1-p)log\\frac{1-p}{1-q} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coding layerì˜ ê° neuronsì— ëŒ€í•´ sparsity lossì„ ê³„ì‚°í–ˆë‹¤ë©´, ì´ lossë“¤ì„ ëª¨ë‘ í•©í•´ì„œ cost functionì˜ ê²°ê³¼ì— ë”í•¨   \n",
    "   \n",
    "sparsity lossì™€ reconstruction lossì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ë¥¼ ì œì–´í•˜ê¸° ìœ„í•´ sparsity lossì— sparsity weight hyperparameterë¥¼ ê³±í•¨   \n",
    "   \n",
    "sparsity weight hyperparameterê°€ ë„ˆë¬´ í¬ë©´ modelì´ ëª©í‘œ saprsityì— ê°€ê¹ê²Œ ë˜ê³   \n",
    "sparsity weight hyperparameterê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ modelì´ sparsityë¥¼ ê±°ì˜ ë¬´ì‹œí•  ê²ƒì´ë¯€ë¡œ ì–´ë–¤ í¥ë¯¸ë¡œìš´ íŠ¹ì„±ë„ í•™ìŠµí•˜ì§€ ëª»í•  ê²ƒ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 1000  # sparse codings\n",
    "n_outputs = n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    # Kullback Leibler divergence\n",
    "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
    "\n",
    "learning_rate = 0.01\n",
    "sparsity_target = 0.1\n",
    "sparsity_weight = 0.2\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])            # not shown in the book\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid) # not shown\n",
    "outputs = tf.layers.dense(hidden1, n_outputs)                     # not shown\n",
    "\n",
    "hidden1_mean = tf.reduce_mean(hidden1, axis=0) # batch mean\n",
    "sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run([reconstruction_loss, sparsity_loss, loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train MSE:\", reconstruction_loss_val, \"\\tSparsity loss:\", sparsity_loss_val, \"\\tTotal loss:\", loss_val)\n",
    "        saver.save(sess, \"./my_model_sparse.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_reconstructed_digits(X, outputs, \"./my_model_sparse.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coding layer must output values from 0 to 1, which is why we use the sigmoid activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up training, you can normalize the inputs between 0 and 1, and use the cross entropy instead of the MSE for the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(hidden1, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\n",
    "reconstruction_loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‘Š Variational Autoencoder (ë³€ì´í˜• Autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¸ê¸° ë§ì€ autoencoderì˜ í•œ ì¢…ë¥˜   \n",
    "   \n",
    "ê¸°ì¡´ Autoencoderì™€ ë‹¤ë¥¸ ì   \n",
    "- Probabilistic Autoencoder. ì¦‰, í›ˆë ¨ì´ ëë‚œ í›„ì—ë„ ì¶œë ¥ì´ ë¶€ë¶„ì ìœ¼ë¡œ ìš°ì—°ì— ì˜í•´ ê²°ì •ë¨  \n",
    "&nbsp; (ì´ì™€ëŠ” ë°˜ëŒ€ë¡œ Denoising AutoencoderëŠ” í›ˆë ¨ ì‹œì—ë§Œ ë¬´ì‘ìœ„ì„±ì„ ì‚¬ìš©)   \n",
    "     \n",
    "     \n",
    "- Generative Autoencoderë¼ëŠ” ê²ƒì´ ì¤‘ìš”.  ë§ˆì¹˜ í›ˆë ¨ setì—ì„œ sampingëœ ê²ƒ ê°™ì€ new sampleì„ ìƒì„± ê°€ëŠ¥  \n",
    "  \n",
    "=> ì´ëŸ¬í•œ ë‘ ì†ì„±ì— ì˜í•´ RBMê³¼ ìœ ì‚¬í•˜ê²Œ ë˜ê³  í›ˆë ¨ì´ ë” ì‰½ê³  sampling ê³¼ì •ì´ í›¨ì”¬ ë¹ ë¦„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Autoencoder(left) & ì´ë¥¼ í†µê³¼í•˜ëŠ” Sample(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Ch 15. Variational Autoencoder(left) & ì´ë¥¼ í†µê³¼í•˜ëŠ” sample(right).jpg\", width = 600, height = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ ê·¸ë¦¼ ì™¼ìª½ì€ Variational Autoencoderë¥¼ ë³´ì—¬ì¤Œ   \n",
    "1. êµ¬ì¡°ëŠ” Encoderì™€ ê·¸ ë’¤ë¥¼ ë”°ë¥´ëŠ” Decoderë¡œ êµ¬ì„±  \n",
    "2. ì°¨ì´ì ì€ ì…ë ¥ì— ëŒ€í•œ codingì„ ë°”ë¡œ ë§Œë“œëŠ” ëŒ€ì‹  encoderê°€ mean coding(í‰ê·  ì½”ë”©) $ \\mu$ì™€ í‘œì¤€í¸ì $\\sigma$ë¥¼ ë§Œë“¦  \n",
    "3. ì‹¤ì œ codingì€ í‰ê· ì´ $\\mu$ì´ê³  í‘œì¤€í¸ì°¨ê°€ $\\sigma$ì¸ Gaussian distributionì—ì„œ randomí•˜ê²Œ samplingëœ í›„ decoderê°€ samplingëœ codingì„ ë³´í†µì²˜ëŸ¼ decoding\n",
    "4. ê·¸ë¦¼ 15-11ì˜ ì˜¤ë¥¸ìª½ì€ autoencoderë¥¼ í†µê³¼í•˜ëŠ” í›ˆë ¨ ìƒ˜í”Œì„ ë³´ì—¬ì¤Œ ë¨¼ì € encoderê°€ $\\mu$ì™€ $\\sigma$ë¥¼ ë§Œë“¤ë©´ codingì´ randomí•˜ê²Œ ì„ íƒë¨  \n",
    "($\\mu$ì˜ ìœ„ì¹˜ì™€ ì •í™•íˆ ê°™ì§€ëŠ” ì•ŠìŒ)  \n",
    "5. ì´ codingì´ decodeë˜ê³  ìµœì¢… ì¶œë ¥ì€ í›ˆë ¨ sampleì„ ë‹®ëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ì…ë ¥ì´ ë§¤ìš° ë³µì¡í•œ ë¶„í¬ë¥¼ ê°€ì§€ë”ë¼ë„ variational autoencoderëŠ” ë§ˆì¹˜ ê°„ë‹¨í•œ gaussian distributionì—ì„œ samplingëœ ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ” codingì„ ë§Œë“œëŠ” ê²½í–¥ìˆìŒ     \n",
    "    \n",
    "    \n",
    "- í›ˆë ¨í•˜ëŠ” ë™ì•ˆ cost functionì´ codingì„ gaussian sampleë“¤ì˜ êµ°ì§‘ì²˜ëŸ¼ ë³´ì´ëŠ” ê±°ì˜ êµ¬ í˜•íƒœë¥¼ ê°€ì§„    \n",
    "  coding space(ì½”ë”© ê³µê°„) (ë˜ëŠ” latent space ì ì¬ ë³€ìˆ˜ ê³µê°„)ìœ¼ë¡œ ì ì§„ì ìœ¼ë¡œ ì´ë™ì‹œí‚¤ë¯€ë¡œ     \n",
    "  variational autoencoderëŠ” í›ˆë ¨ì´ ëë‚œ ë’¤ new sampleì„ ë§¤ìš° ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë¨    \n",
    "   \n",
    "    \n",
    "- Gaussian Distributionë¡œë¶€í„° randomí•œ codingì„ samplingí•´ decodingí•˜ë©´ ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost functionì€ ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±   \n",
    "   \n",
    "  1. Autoencoderê°€ ì…ë ¥ì„ ì¬ìƒì‚°í•˜ë„ë¡ ë§Œë“œëŠ” ì¼ë°˜ì ì¸ reconstruction loss(ì¬êµ¬ì„± ì†ì‹¤), ì•ì—ì„  ì—¬ê¸°ì— cross entropyë¥¼ ì‚¬ìš©\n",
    "  2. Gaussian Distributionì—ì„œ sampleëœ ê²ƒ ê°™ì€ codingì„ ê°€ì§€ë„ë¡ autoencoderë¥¼ ê°•ì œí•˜ëŠ” latent loss(ì ì¬ ë³€ìˆ˜ ì†ì‹¤)  \n",
    "  3. ëª©í‘œ ë¶„í¬(Gaussian Distribyution)ì™€ ì‹¤ì œ coding distribution ì‚¬ì´ì˜ KL Divergenceì„ ì‚¬ìš©  \n",
    "  4. coding layerìœ¼ë¡œ ì „ë‹¬ë  ìˆ˜ ìˆëŠ” ì •ë³´ ì–‘ì„ ì œí•œí•¨ìœ¼ë¡œì¨ autoencoderê°€ ìœ ìš©í•œ íŠ¹ì„±ì„ í•™ìŠµí•˜ê²Œ ë§Œë“œëŠ” gaussian noise ë•Œë¬¸ì— ìˆ˜ì‹ì´ ë³µì¡."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 20  # codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.001\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense,\n",
    "    activation=tf.nn.elu,\n",
    "    kernel_initializer=initializer)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "hidden3_sigma = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)\n",
    "hidden3 = hidden3_mean + hidden3_sigma * noise\n",
    "hidden4 = my_dense_layer(hidden3, n_hidden4)\n",
    "hidden5 = my_dense_layer(hidden4, n_hidden5)\n",
    "logits = my_dense_layer(hidden5, n_outputs, activation=None)\n",
    "outputs = tf.sigmoid(logits)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\n",
    "reconstruction_loss = tf.reduce_sum(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-10 # smoothing term to avoid computing log(0) which is NaN\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.square(hidden3_sigma) + tf.square(hidden3_mean)\n",
    "    - 1 - tf.log(eps + tf.square(hidden3_sigma)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = reconstruction_loss + latent_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_val, reconstruction_loss_val, latent_loss_val = sess.run([loss, reconstruction_loss, latent_loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val, \"\\tReconstruction loss:\", reconstruction_loss_val, \"\\tLatent loss:\", latent_loss_val)\n",
    "        saver.save(sess, \"./my_model_variational.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 20  # codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.001\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "my_dense_layer = partial(\n",
    "    tf.layers.dense,\n",
    "    activation=tf.nn.elu,\n",
    "    kernel_initializer=initializer)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "hidden3_gamma = my_dense_layer(hidden2, n_hidden3, activation=None)\n",
    "noise = tf.random_normal(tf.shape(hidden3_gamma), dtype=tf.float32)\n",
    "hidden3 = hidden3_mean + tf.exp(0.5 * hidden3_gamma) * noise\n",
    "hidden4 = my_dense_layer(hidden3, n_hidden4)\n",
    "hidden5 = my_dense_layer(hidden4, n_hidden5)\n",
    "logits = my_dense_layer(hidden5, n_outputs, activation=None)\n",
    "outputs = tf.sigmoid(logits)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\n",
    "reconstruction_loss = tf.reduce_sum(xentropy)\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)\n",
    "loss = reconstruction_loss + latent_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model and generate a few random digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_digits = 60\n",
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\") # not shown in the book\n",
    "            sys.stdout.flush()                                          # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        loss_val, reconstruction_loss_val, latent_loss_val = sess.run([loss, reconstruction_loss, latent_loss], feed_dict={X: X_batch}) # not shown\n",
    "        print(\"\\r{}\".format(epoch), \"Train total loss:\", loss_val, \"\\tReconstruction loss:\", reconstruction_loss_val, \"\\tLatent loss:\", latent_loss_val)  # not shown\n",
    "        saver.save(sess, \"./my_model_variational.ckpt\")  # not shown\n",
    "    \n",
    "    codings_rnd = np.random.normal(size=[n_digits, n_hidden3])\n",
    "    outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,50)) # not shown in the book\n",
    "for iteration in range(n_digits):\n",
    "    plt.subplot(n_digits, 10, iteration + 1)\n",
    "    plot_image(outputs_val[iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 6\n",
    "n_cols = 10\n",
    "plot_multiple_images(outputs_val.reshape(-1, 28, 28), n_rows, n_cols)\n",
    "save_fig(\"generated_digits_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the latent loss is computed differently in this second variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode & Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits = 3\n",
    "X_test, y_test = mnist.test.next_batch(batch_size)\n",
    "codings = hidden3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_variational.ckpt\")\n",
    "    codings_val = codings.eval(feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_variational.ckpt\")\n",
    "    outputs_val = outputs.eval(feed_dict={codings: codings_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the reconstructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 2.5 * n_digits))\n",
    "for iteration in range(n_digits):\n",
    "    plt.subplot(n_digits, 2, 1 + 2 * iteration)\n",
    "    plot_image(X_test[iteration])\n",
    "    plt.subplot(n_digits, 2, 2 + 2 * iteration)\n",
    "    plot_image(outputs_val[iteration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_iterations = 3\n",
    "n_digits = 6\n",
    "codings_rnd = np.random.normal(size=[n_digits, n_hidden3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_variational.ckpt\")\n",
    "    target_codings = np.roll(codings_rnd, -1, axis=0)\n",
    "    for iteration in range(n_iterations + 1):\n",
    "        codings_interpolate = codings_rnd + (target_codings - codings_rnd) * iteration / n_iterations\n",
    "        outputs_val = outputs.eval(feed_dict={codings: codings_interpolate})\n",
    "        plt.figure(figsize=(11, 1.5*n_iterations))\n",
    "        for digit_index in range(n_digits):\n",
    "            plt.subplot(1, n_digits, digit_index + 1)\n",
    "            plot_image(outputs_val[digit_index])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Autoencoder ì¢…ë¥˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìˆ˜ì¶• ì˜¤í† ì¸ì½”ë” (Contractive Autoencoder, CAE)\n",
    "  - í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì…ë ¥ì— ëŒ€í•œ codingì˜ ë³€í™”ìœ¨ì´ ì‘ë„ë¡ ì œì•½ì„ ë°›ìŒ    \n",
    " (ë‘ ê°œì˜ ë¹„ìŠ·í•œ ì…ë ¥ì€ ë¹„ìŠ·í•œ codingì´ ë˜ì–´ì•¼í•¨)   \n",
    "   \n",
    "   <p>   \n",
    "- ì ì¸µ í•©ì„±ê³± ì˜¤í† ì¸ì½”ë” (Stacked Convoluional Autoencoders)\n",
    "  - Convolution layerì„ í†µí•´ ì²˜ë¦¬ë˜ëŠ” imageë¥¼ ì¬êµ¬ì„±í•¨ìœ¼ë¡œì¨ ì‹œê°ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë²•ì„ í•™ìŠµ   \n",
    "     \n",
    "   <p>\n",
    "- í™•ë¥ ì  ìƒì„± ë„¤íŠ¸ì›Œí¬ (Generative Stochastic Network, GSN)\n",
    "  - Dataë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì¶”ê°€í•œ denoise autoencoderì˜ ì¼ë°˜í™”ëœ model  \n",
    "  \n",
    "  <p>\n",
    "- WTA ì˜¤í† ì¸ì½”ë” (Winner-Take-All Autoencoder)\n",
    "  - í›ˆë ¨í•˜ëŠ” ë™ì•ˆ coding layerì— ìˆëŠ” ëª¨ë“  neuronsì˜ í™œì„±í™”ë¥¼ ê³„ì‚°í•œ í›„ í›ˆë ¨ ë°°ì¹˜ì—ì„œ ê° neuronsì— ëŒ€í•´ ìµœëŒ€ $k%$í™œì„±í™”ë§Œ ë³´ì¡´í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ì„¤ì •.  \n",
    "  - ìì—°ìŠ¤ëŸ½ê²Œ sparsity codingì„ ë§Œë“¤ê³  ë¹„ìŠ·í•œ ë°©ì‹ìœ¼ë¡œ WTA ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ sparsity convolution autoencoderë¥¼ ìƒì„±  \n",
    "     \n",
    "   <p> \n",
    "- ì ëŒ€ì  ìƒì„± ë„¤íŠ¸ì›Œí¬ (Generative Adversarial Network, GAN)\n",
    "  - 'discriminator'(íŒë³„ì)ë¼ ë¶ˆë¦¬ëŠ” networkê°€ 'generator'(ìƒì„±ì)ë¼ê³  ë¶ˆë¦¬ëŠ” ë‘ ë²ˆì§¸ newworkê°€ ë§Œë“  ê°€ì§œ dataì™€ ì‹¤ì œ dataë¥¼ êµ¬ë¶„í•˜ë„ë¡ í›ˆë ¨ë¨\n",
    "  - generatorëŠ” discriminatorë¥¼ ì†ì´ëŠ” ë²•ì„ í•™ìŠµí•˜ë©°, discriminatorëŠ” generatorì˜ ì†ì„ìˆ˜ë¥¼ í”¼í•˜ëŠ” ë²•ì„ í•™ìŠµ\n",
    "  - ì´ëŸ° ê²½ìŸì€ ë§¤ìš° í˜„ì‹¤ì ì¸ ê°€ì§œ dataì™€ ì•ˆì •ì ì¸ codingì„ ìƒì„±í•˜ë„ë¡ ë§Œë“¦\n",
    "  - ì ëŒ€ì  í›ˆë ¨ì€ ë§ì€ ê´€ì‹¬ì„ ë°›ê³  ìˆìŒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
