{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 10 인경신경망 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인경 신경망(Artificial neural networks, ANN)은 뇌의 구조에서 영감을 받은 알고리즘 종류이다. 뇌의 뉴런과 같이 무수히 많은 뉴런을 통해\n",
    "사람처럼 복잡한 의사결정을 내릴 수 있다는 아이디어이다. 이를 통해서, 오늘날의 구글 검색엔진과 추천 알고리즘, 대표적으로 알파고 등이\n",
    "존재한다. 설명을 위해 김성훈 교수님의 모두의 딥러닝을 사용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](C:\\Users\\heon1\\ml_handson\\images\\10\\1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/10/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/10/1.png)\n",
    "뇌의 구조와 같이 정보를 받았을 때 정보를 취합하여, 일정치를 넘을 때 다음 \n",
    "신경으로 넘긴다. 이는 아래와 같이 데이터를 취합하여 활성화 함수를 통해 값을\n",
    "출력하는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 뉴런을 무수히 많이 여러층으로 논 것이 신경망이라고 볼 수 있다. 밑의 그림처럼 \n",
    "간단한 아이디어 같지만, 무수히 많은 뉴런이 있을 때 뛰어난 결과를 보인다. \n",
    "![](./images/10/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/10/3.png)\n",
    "\n",
    "신경망은 처음 발표시 단층의 경우 선형의 조합이었으므로, xor같은 비선형 문제를 \n",
    "풀 수 없었고, 이러한 한계점으로 사용되지 않았다. 하지만 Layer를 한층 더\n",
    "늘려 비선형성을 추가할 때, 이를 극복하여 가능하게 되었다.\n",
    "![](./images/10/4.png)\n",
    "우리는 다층 신경망의 대표적인 뉴럴넷 MultilayerPerceptron, MLP를 간단히 실습해 볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.3 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heon1\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\heon1\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\heon1\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set()\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 한글출력\n",
    "# matplotlib.rc('font', family='AppleGothic')  # MacOS\n",
    "matplotlib.rc('font', family='Malgun Gothic')  # Windows\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(random_state = 42)\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]]) # sklearn에서 지원하지만 실제론 쓰이지 않는다.\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로의 저수준 api로 심층 신경망 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 대표적인 신경망 데이터셋인 mnist를 통하여, 간단하게 mlp를 구현해보는 것을 \n",
    "목표로 한다. 코드는 https://github.com/sjchoi86/Tensorflow-101/blob/master/notebooks/mlp_mnist_simple.ipynb\n",
    "에서 가져왔다. 우리의 mlp구조는 아래와같다.\n",
    "![신경망](Desktop\\스터디공부\\10장\\5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packages loaded\n",
      "tf version is1.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"packages loaded\")\n",
    "print(\"tf version is%s\" % (tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "mnust loaded\n"
     ]
    }
   ],
   "source": [
    "# mnist\n",
    "mnist = input_data.read_data_sets('data/', one_hot=True)\n",
    "mnist\n",
    "trainimg = mnist.train.images\n",
    "trainlabel = mnist.train.labels\n",
    "testimg = mnist.test.images\n",
    "testlabel = mnist.test.labels\n",
    "print(\"mnust loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000021A953F5FC8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000021A96ECBF88>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000021A96ECBE88>)\n",
      "(55000, 784)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(mnist)\n",
    "print(trainimg.shape)\n",
    "print(trainlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK TOPOLOGIES\n",
    "n_hideen_1 = 256\n",
    "n_hideen_2 = 128\n",
    "n_input = 784\n",
    "n_classes = 10\n",
    "epochs = 50\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "# INPUTS AND OUTPUTS\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NETWORK PARAMETERS\n",
    "stddev = 0.1\n",
    "weights = {\n",
    "    'h1' : tf.Variable(tf.random_normal([n_input, n_hideen_1], stddev=stddev)),\n",
    "    'h2' : tf.Variable(tf.random_normal([n_hideen_1, n_hideen_2], stddev=stddev)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hideen_2, n_classes], stddev=stddev))\n",
    "}\n",
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([n_hideen_1])),\n",
    "    'b2' : tf.Variable(tf.random_normal([n_hideen_2])),\n",
    "    'out' : tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP AS FUNCTION\n",
    "def multilayer_perceptron(_X, _weights, _biases) :\n",
    "    layer1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1']))\n",
    "    layer2 = tf.nn.relu(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2']))\n",
    "    return (tf.matmul(layer2, _weights['out'])+ _biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-25-afb8759b368b>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits= pred))\n",
    "optm = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "corr = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003/020 cost: 0.055342026\n",
      "TRAIN ACCURACY: 0.990\n",
      "TEST ACCURACY: 0.963\n",
      "Epoch: 007/020 cost: 0.038997449\n",
      "TRAIN ACCURACY: 0.990\n",
      "TEST ACCURACY: 0.967\n",
      "Epoch: 011/020 cost: 0.030459012\n",
      "TRAIN ACCURACY: 0.980\n",
      "TEST ACCURACY: 0.970\n",
      "Epoch: 015/020 cost: 0.029607711\n",
      "TRAIN ACCURACY: 1.000\n",
      "TEST ACCURACY: 0.970\n",
      "Epoch: 019/020 cost: 0.019694811\n",
      "TRAIN ACCURACY: 0.960\n",
      "TEST ACCURACY: 0.970\n",
      "OPTIMIZATION FINISHED\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "training_epochs = 20\n",
    "batch_size      = 100\n",
    "display_step    = 4\n",
    "# LAUNCH THE GRAPH\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# OPTIMIZE\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # ITERATION\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        avg_cost += sess.run(cost, feed_dict=feeds)\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    # DISPLAY\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "        feeds = {x: batch_xs, y: batch_ys}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"TRAIN ACCURACY: %.3f\" % (train_acc))\n",
    "        feeds = {x: mnist.test.images, y: mnist.test.labels}\n",
    "        test_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\"TEST ACCURACY: %.3f\" % (test_acc))\n",
    "print (\"OPTIMIZATION FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝은 많은 하이퍼파라미터를 가지고 있으며, 이들의 조정을 통해\n",
    "학습 성과와 속도의 개선을 가져올 수 있다. 간단히 앞선 모델을 수정하기 위해서\n",
    "그림에서 설명하면 사용되었던 relu를 sigmoid function으로 변경할 수 있다.\n",
    "추가로 비선형성의 추가를 위해 layer를 늘리고 파라미터 수도 자연스럽게 늘어났다.\n",
    "물론, 이러한 수정은 학습의 성과를 낮출 가능성이 크므로, 상황에 따라 조정하는 것이 적절하다.\n",
    "![](./images/10/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가로 사용된 하이퍼 PARAMETERS 중\n",
    "\n",
    "training_epochs = 20\n",
    "\n",
    "batch_size      = 100\n",
    "\n",
    "display_step    = 4\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "optimizer를 adam이 아닌 momentum과 같은 다른 알고리즘으로 변환 할 수 있으며,\n",
    "dropout과 같은 기법을 추가하는 등 다양한 방법이 존재한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
