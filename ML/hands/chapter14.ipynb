{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14 순환신경망(RNN, Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 챕터에서는 sequence라는 특징을 가지고 있는 rnn에 대해서 살펴볼 것이다.\n",
    "\n",
    "rnn의 가장 큰 특징은 기존 cnn과 ann은 고정된 입력데이터와 출력값을 가지는 반면, rnn은 임의의 길이를 가진 sequence데이터에\n",
    "유용하며 이는, 입력과 출력이 가변적일 수 있는것을 의미한다.\n",
    "두번째로, rnn은 이전 학습들이 다음 학습들에 영향을 주므로 , 즉 시간의 순서를 가지고 있는 데이터에 유용하다.\n",
    "이에 따라 사용되는 종류로, 기계번역과 비디오, 음악, 주가 등 순서를 가지는(이전의 내용이 다음 내용에 영향을 주는) 데이터 분석에 쓰이며,\n",
    "이는 다양하고 창의적인 활동이 가능하게 만들어준다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn의 구조는 다음과 같다. \n",
    "그림과 같이 이전 hiddenstate의 값이 현재의 hidden state값에 영향을 주는 것을 볼 수 있다.\n",
    "기존의 우리가 봐왔던 신경망은 입력값에 대한 Weights를 가지지만 RNN은 이와 다르게\n",
    "입력의 Weights와 이전 hiddenstate의 Weights, 그리고 y에 대한 Weights가 있다는 것을 알 수 있다.\n",
    "즉 이와같이 이전 학습 상태를 현재 학습에 사용함으로 이전 데이터의 순서가 포함된다고 할 수 있다.\n",
    "![신경망](./images/10/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![신경망](Desktop\\스터디공부\\10장\\15.png){: width=\"100\" height=\"100\"}\n",
    "위의 그림을 보면 첫번째 사진은 사진 한장에 대한 문장을 출력하는 구조이며, 두번째는 여러게의 단어들인 문장을 입력으로 사용하여\n",
    "이 문장의 감성점수를 파악하는 것이다. 마지막으로는 문장을 받아 번역한 문장을 출력하는 구조 등을 간단히 도식화한 것이다.\n",
    "물론 밑에 그림에서 그려놨듯이, 그림에 존재하지 않을뿐 실제로 비어있는 부분에도 출력과 입력이 존재한다. \n",
    "하지만 우리가 원하는 목표값과 우리가 의도한 입력값만을 표시하기 위해 그 외는 빈 공간으로 되어있는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![신경망](Desktop\\스터디공부\\10장\\16.png)\n",
    "앞서 RNN의 Sequence가 굉장히 긴 문장이라면 어떻게 될까?? \n",
    "여러번의 activation function을 지나면서 처음에 있던 input과 hiddenstate의 weights들이 점차 작아지며, \n",
    "이에 따라 처음 입력의 가중치가 사라질 것이다.\n",
    "예를 들어 영화의 리뷰가 \"환상적이다. 이렇게 사람이 짜증나고 식은땀이나서 죽어버릴 것만 같다.\" 라는 글은 짧긴하지만, \n",
    "\"환상적\"이라는 말이 중요한데 이 단어의 가중치가 점점 작아지고 뒤에 글을 중요하게 여겨 부정적인 리뷰로 인식할 수 있다.\n",
    "이처럼 rnn은 sequence의 길이에 대한 한계점을 가지고 있으며, 이에 대하여 보안한 것이 LSTM(Long Short Term Memory)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM은 Long Short Term Memory의 약자로, rnn의 한계점으로 지적되었던 긴문장에서 초기의 입력값들도 나중에도 영향력을\n",
    "유지할 수 있게 하기위해 고안되었다. 물론 RNN의 한계를 보완하였다는 것이지 LSTM도 동일한 문제를 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![신경망](Desktop\\스터디공부\\10장\\17.png)\n",
    "LSTM은 Cell State라는 컨베이어 벨트를 가지고 있으며, 이에 대해서\n",
    "forget gate, input gate, output gate를 사용하여, 학습의 안정성을 높이려 하였다.\n",
    "\n",
    "forget gate는 sigmoid를 지나고 난 후의 값과 cell state를 곱한다. 이는 sigmoid 0~1 사이의 값을 통해 이전 cell state중 어떤 값을\n",
    "0에 가깝게 그대로 유지할지를 정하는 것과 유사하다. 즉, 이전 cell sate 중 어떤 값들을 forget할지 정하는 것이다.\n",
    "\n",
    "input gate의 경우 g는 기존 우리가 보았던 rnn과 동일하며, input에서 sigmoid를 통해 이번 값의 중요한 부분을 설정하고 곱하는 것과 유사하다.\n",
    "그 후 이 값을 forget gate를 지난 cell state에 더하여 준다.\n",
    "\n",
    "이를 통해 Cell State값을 통해 output을 도출해 내는 것으로, 컨베이어 벨트의 cell state가 존재함에 따라,\n",
    "학습의 안정성이 지속되는 것으로 파악할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn을 통한 번역을 위해서는 seq2seq, wordtovector, attention 등 기법에 대한 이해와 지식이 필요하며, 본인은 그런 능력도 없다.\n",
    "그리고 있다고 한들 이것까지 설명하려면 너무 오래걸려서 각자하시길 바란다.\n",
    "\n",
    "https://github.com/sjchoi86/tensorflow-101/blob/master/notebooks/rnn_mnist_simple.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
