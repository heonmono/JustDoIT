{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공 신경망 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10장에서는 인공신경망의 간단한 아이디어와 발전과정을 살펴볼 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망(ANN, Aritificial Neural Networks)은 1943년 신경생리학자 Warren McCulloch과 수학자 Walter Pitts가 'A Logical Calculus of Ideas Immanent In Nervous Activity' 처은 소개했으며, 명제 논리(propositional logic)를 사용해 동물 뇌의 생물학적 뉴런이 복잡한 계산을 위해 어떻게 상호작용하는지에 대해 간단한 계산 모델을 제시했다.\n",
    "\n",
    "1960년대까지는 이렇게 등장한 인공 신경망을 통해 사람들은 지능을 가진 기계와 대화를 나눌 수 있을 것이라고 생각했다. 하지만 아래 그림(출처: beamandrew's blog)처럼 사람들의 기대와는 달리 인공 신경망으로 XOR문제를 해결할 수 없게 되었고, 1990년 대에는 SVM과 성능이 좋은 다른 머신러닝 알고리즘들이 나오게 되면서 인공 신경망은 암흑기로 접어 들게 되었다.\n",
    "\n",
    "\n",
    "![](./images/history.jpg)\n",
    "\n",
    "\n",
    "\n",
    "2000년 대에 들어서면서 인공 신경망은 2012년 ILSVRC2012 대회에서 인공 신경망을 깊게 쌓은 딥러닝 모델인 AlexNet이 압도적인 성적으로 우승하면서 다시금 주목받게 되었다. 이렇게 인공 신경망(딥러닝)이 다시 주목받게 된 계기는 다음과 같은 것들이 있다.\n",
    "\n",
    "- 빅 데이터 시대인 요즘 신경망을 학습시키기 위한 데이터가 엄청나게 많아 졌다.\n",
    "- 신경망은 다른 머신러닝 알고리즘보다 규모가 크고 복잡한 문제에서 성능이 좋다.\n",
    "- 1990년대 이후 크게 발전된 컴퓨터 하드웨어 성능과 Matrix연산에 고성능인 GPU로 인해 상대적으로 짧은 시간 안에 대규모의 신경망을 학습시킬 수 있게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 생물학적 뉴런을 살펴볼 필요가 있다. 별로 중요하지는 않지만 이는 신경망의 아이디어와 비슷하기 때문이다.\n",
    "- **Dendrite** : 수상돌기, 다른 뉴런으로부터 신호를 수용하는 부분\n",
    "- **Axon** : 축삭돌기, 신호를 내보내는 부분\n",
    "- **Synaptic terminals** : 시냅스(synapse) 뉴런의 접합부, 다른 뉴런으로 부터 짧은 전기 자극 **신호**(signal)를 받음\n",
    "​\n",
    "![](./images/neuron.png)\n",
    "\n",
    "우리의 뇌는 수상돌기에서 데이터를 받아 다음 수상돌기의 synapse에서 이전 정보를 활성화 유무를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.1.3 퍼셉트론\n",
    "퍼셉트론(Perceptron)은 Frank Rosenblatt가 1975년에 제안한 인공 신경망 구조 중 하나이며, 이 퍼셉트론이 바로 신경망(딥러닝)의 기원이 되는 알고리즘이라고 할 수 있다. 퍼셉트론에 대한 자세한 내용은 여기서 확인할 수 있다.\n",
    "\n",
    "퍼셉트론은 TLU(Threshold Logic Unit)이라는 형태의 뉴런을 기반으로 하며, 아래의 그림과 같이 입력과 출력이 어떤 숫자고 각각의 입력에 각각 고유한 가중치( 𝐖 , weight)가 곱해진다.\n",
    "𝑧=𝑤1𝑥1+𝑤2𝑥2+⋯+𝑤𝑛𝑥𝑛=𝐖𝑇⋅𝐗\n",
    " \n",
    "그런 다음 계산된 합  𝑧 에 계단 함수(step function)를 적용하여 결과  ℎ 를 출력한다.\n",
    "![](./images/perceptron02.png)\n",
    "가중치의 합까지는 우리가 기존에 배웠던 선형함수와 동일하다. 하지만 앞서 신경망을 통해 보았던\n",
    "시냅스 부분인 계단함수가 존재한다. 계단함수는 가중치 합이 일정치를 넘을 경우 활성화 하며, 그렇지 않으면\n",
    "비활성화된다. 계단함수를 통해 이진 분류가 가능하다.\n",
    "이러한 퍼셉트론을 3개로 구성하면 이 퍼셉트론은 샘플 세 개의 클래스(레이블)로 분류할 수 있는 Multioutput Classifier이다.\n",
    "\n",
    "![](./images/multi-tlu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 퍼셉트론의 약점: XOR 문제\n",
    "\n",
    "1969년 Marvin Minsky와 Seymour Papert는 '퍼셉트론'이란 논문에서 퍼셉트론의 심각한 약점이 있다는 것을 보였는데, 그 중에서도 가장 결정적인 것은 선형결합인 퍼셉트론이 배타적 논리합인 XOR 분류 문제를 해결할 수 없다는 것이었다. \n",
    "\n",
    "![or-vs-xor](./images/or-vs-xor.png)\n",
    "단층으로 퍼셉트론은 선형의 조합과 동일하며 이는 xor과 같은 간단한 문제를 해결할 수 없었으며,\n",
    "이에 따라서 SVM과 RANDOMFOREST와 같이 더 간단하면서 강력한 머신러닝 기법들이 주목 받고, ANN에 대한 한계로 발전이 멈추었다.\n",
    "\n",
    "\n",
    "이를 계기로 인공 신경망은 암흑기를 맞이하게 되었다. 하지만, 단일 퍼셉트론을 여러개 쌓아 **다층 퍼셉트론**(**MLP**, Multi-Layer Perceptron)을 통해 XOR 분류 문제를 해결할 수 있었다. \n",
    "아래와 같이 Layer가 1개에서 2개로 늘어났을 때, 더 복잡한 비선형성을 가질 수 있었고 이를 통해 XOR문제를 해결할 수 있었다.\n",
    "![](./images/xor_gate05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파를 통해 w와 b를 학습할수 있으며, \n",
    "다층의 레이어와 퍼셉트론으로 xor과 같이 복잡한 문제를 해결할 수 있다는 것을 \n",
    "우리는 지금까지 배웠다.\n",
    "하지만 여기서 새로운 문제점이 발견되었는데 기존까지는 sigmoid를 activation function으로 \n",
    "사용하였는데, 이에 대해서 layer를 깊게 할 경우\n",
    "vanishing gradient가 발생하였다.\n",
    "vanishing gradient는 역전파 과정에서 sigmoid의 미분을 여러번 거치어, 앞쪽에 있는\n",
    "w가 update가 되지 않고 변하지 않는 것을 의미한다. \n",
    "신경망은 많은 layer를 통해 비선형성과 복잡한 문제를 풀 수 있는 것이 강점을 가지는 것인데\n",
    "빈선형성을 가지는데 한계를 가지므로, 신경망에 대한 관심이 줄어드는 계기였다.\n",
    "![](./images/vanishing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 활성화 함수 (activation function)\n",
    "\n",
    "역전파 알고리즘이 잘 동작하기 위해서 다층 퍼셉트론(MLP)의 구조에 변화를 주었는 데, 그것이 바로 활성화 함수 부분에서 계단 함수를 시그모이드 함수(로지스틱 함수)로 바꿔준 것이다. 이렇게 활성화 함수를 시그모이드 함수로 바꿔준 이유는 가중치 매개변수를 조정 해주기 위해 그래디언트, 편미분을 계산하게 되는데, 계단 함수는 0을 기준으로 기울기가 없는 직선이므로 그래디언트를 계산하는 것이 의미가 없기 때문이다(0을 기준으로 불연속이기 때문에 미분이 불가능한 이유도 있다).  활성화 함수로는 아래의 그림처럼 로지스틱 함수 외에 다양한 활성화 함수를 사용할 수 있다.\n",
    "\n",
    "![](./images/activation02.png)\n",
    "\n",
    "\n",
    "\n",
    "위의 그림에서 **ReLU** 또한 0에서 연속이지만 첩점(뾰족한 점)이므로 미분이 불가능하다. 하지만 0보다 큰 경우에는 미분을 적용하고 0 이하인 값에는 0을 줌으로써 해결할 수 있다. 또한, ReLU가 성능이 좋을 뿐만아니라 'cs231n' 강의에서는 실제 생물학적으로 시그모이드 보다 그럴듯한 작용을 한다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"스텝\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2, label=\"로지스틱\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"활성화 함수\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(logit, z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.title(\"도함수\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid와 비슷한 tanh을 보았을때, 도함수가 대부분 0 주위에 분포하는 것을 파악할 수 있다.\n",
    "이렇듯 기존의 sigmoid는 미분결과 0에 근접하였으며, layer가 여러층이 있을 경우\n",
    "미분값이 0에 근접하여 vanishing gradient가 발생했다는 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소프트맥스(softmax) 함수\n",
    "\n",
    "**소프트맥스 함수**(softmax function)는 출력층에서 주로 사용하는 활성화 함수이며, 식은 다음과 같다.\n",
    "$$\n",
    "\\hat{y}_k = \\frac{ \\text{exp} \\left( \\mathbf{W}^{T} \\cdot \\mathbf{x} \\right)}{\\sum_{j=1}^{K}{\\text{exp} \\left( \\mathbf{W}^{T} \\cdot \\mathbf{x} \\right)}} \\quad (K=\\text{# of class})\n",
    "$$\n",
    "\n",
    "\n",
    "소프트맥스 함수의 특징은 출력값의 총합이 1이 된다는 것이다. 따라서, 각 출력 뉴런에 대한 소프트맥스의 출력값은 각 클래스에 대응하는 추정 확률값으로 볼 수 있다.\n",
    "총합이 1로 정규화되는 장점과 동시에, 자연함수의 특징으로 인하여, 각 클래스의 확률의 차이가 증가하며, 이는 비용함수에서 맞춘것에 대한 정확도는 더 높게 판단하고 틀린 것의 차이는 더 크게 만들어주는 장점을 가진다.\n",
    "\n",
    "![](./images/softmax.png)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist데이터로 ann 실습\n",
    "![]("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/mmmnnn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MNIST Dataset Load!\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape : 28 x 28 -> 784\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# split validation set\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################\n",
    "# layer params #\n",
    "################\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"inputs\")\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(inputs=inputs, units=n_hidden1,\n",
    "                              activation=tf.nn.relu, name='hidden1',\n",
    "                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu, name='hidden2',\n",
    "                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Hyper-params #\n",
    "################\n",
    "learning_rate = 0.01\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(predictions=logits, targets=labels, k=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(train_op, feed_dict={inputs: X_batch,\n",
    "                                          labels: y_batch})\n",
    "        \n",
    "        acc_batch = accuracy.eval(feed_dict={inputs: X_batch, labels: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={inputs: X_valid, labels: y_valid})\n",
    "        print('epoch: {:03d}, bacth acc: {:.4f}, valid acc: {:.4f}'.format(epoch,\n",
    "                                                                          acc_batch,\n",
    "                                                                          acc_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
