{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì¸ê³µ ì‹ ê²½ë§ ì†Œê°œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10ìž¥ì—ì„œëŠ” ì¸ê³µì‹ ê²½ë§ì˜ ê°„ë‹¨í•œ ì•„ì´ë””ì–´ì™€ ë°œì „ê³¼ì •ì„ ì‚´íŽ´ë³¼ ì˜ˆì •ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¸ê³µ ì‹ ê²½ë§(ANN, Aritificial Neural Networks)ì€ 1943ë…„ ì‹ ê²½ìƒë¦¬í•™ìž Warren McCullochê³¼ ìˆ˜í•™ìž Walter Pittsê°€ 'A Logical Calculus of Ideas Immanent In Nervous Activity' ì²˜ì€ ì†Œê°œí–ˆìœ¼ë©°, ëª…ì œ ë…¼ë¦¬(propositional logic)ë¥¼ ì‚¬ìš©í•´ ë™ë¬¼ ë‡Œì˜ ìƒë¬¼í•™ì  ë‰´ëŸ°ì´ ë³µìž¡í•œ ê³„ì‚°ì„ ìœ„í•´ ì–´ë–»ê²Œ ìƒí˜¸ìž‘ìš©í•˜ëŠ”ì§€ì— ëŒ€í•´ ê°„ë‹¨í•œ ê³„ì‚° ëª¨ë¸ì„ ì œì‹œí–ˆë‹¤.\n",
    "\n",
    "1960ë…„ëŒ€ê¹Œì§€ëŠ” ì´ë ‡ê²Œ ë“±ìž¥í•œ ì¸ê³µ ì‹ ê²½ë§ì„ í†µí•´ ì‚¬ëžŒë“¤ì€ ì§€ëŠ¥ì„ ê°€ì§„ ê¸°ê³„ì™€ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìžˆì„ ê²ƒì´ë¼ê³  ìƒê°í–ˆë‹¤. í•˜ì§€ë§Œ ì•„ëž˜ ê·¸ë¦¼(ì¶œì²˜: beamandrew's blog)ì²˜ëŸ¼ ì‚¬ëžŒë“¤ì˜ ê¸°ëŒ€ì™€ëŠ” ë‹¬ë¦¬ ì¸ê³µ ì‹ ê²½ë§ìœ¼ë¡œ XORë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ê²Œ ë˜ì—ˆê³ , 1990ë…„ ëŒ€ì—ëŠ” SVMê³¼ ì„±ëŠ¥ì´ ì¢‹ì€ ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ë‚˜ì˜¤ê²Œ ë˜ë©´ì„œ ì¸ê³µ ì‹ ê²½ë§ì€ ì•”í‘ê¸°ë¡œ ì ‘ì–´ ë“¤ê²Œ ë˜ì—ˆë‹¤.\n",
    "\n",
    "\n",
    "![](./images/history.jpg)\n",
    "\n",
    "\n",
    "\n",
    "2000ë…„ ëŒ€ì— ë“¤ì–´ì„œë©´ì„œ ì¸ê³µ ì‹ ê²½ë§ì€ 2012ë…„ ILSVRC2012 ëŒ€íšŒì—ì„œ ì¸ê³µ ì‹ ê²½ë§ì„ ê¹Šê²Œ ìŒ“ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì¸ AlexNetì´ ì••ë„ì ì¸ ì„±ì ìœ¼ë¡œ ìš°ìŠ¹í•˜ë©´ì„œ ë‹¤ì‹œê¸ˆ ì£¼ëª©ë°›ê²Œ ë˜ì—ˆë‹¤. ì´ë ‡ê²Œ ì¸ê³µ ì‹ ê²½ë§(ë”¥ëŸ¬ë‹)ì´ ë‹¤ì‹œ ì£¼ëª©ë°›ê²Œ ëœ ê³„ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ ìžˆë‹¤.\n",
    "\n",
    "- ë¹… ë°ì´í„° ì‹œëŒ€ì¸ ìš”ì¦˜ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„°ê°€ ì—„ì²­ë‚˜ê²Œ ë§Žì•„ ì¡Œë‹¤.\n",
    "- ì‹ ê²½ë§ì€ ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ ê·œëª¨ê°€ í¬ê³  ë³µìž¡í•œ ë¬¸ì œì—ì„œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.\n",
    "- 1990ë…„ëŒ€ ì´í›„ í¬ê²Œ ë°œì „ëœ ì»´í“¨í„° í•˜ë“œì›¨ì–´ ì„±ëŠ¥ê³¼ Matrixì—°ì‚°ì— ê³ ì„±ëŠ¥ì¸ GPUë¡œ ì¸í•´ ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ì‹œê°„ ì•ˆì— ëŒ€ê·œëª¨ì˜ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìžˆê²Œ ë˜ì—ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê²½ë§ì€ ìƒë¬¼í•™ì  ë‰´ëŸ°ì„ ì‚´íŽ´ë³¼ í•„ìš”ê°€ ìžˆë‹¤. ë³„ë¡œ ì¤‘ìš”í•˜ì§€ëŠ” ì•Šì§€ë§Œ ì´ëŠ” ì‹ ê²½ë§ì˜ ì•„ì´ë””ì–´ì™€ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n",
    "- **Dendrite** : ìˆ˜ìƒëŒê¸°, ë‹¤ë¥¸ ë‰´ëŸ°ìœ¼ë¡œë¶€í„° ì‹ í˜¸ë¥¼ ìˆ˜ìš©í•˜ëŠ” ë¶€ë¶„\n",
    "- **Axon** : ì¶•ì‚­ëŒê¸°, ì‹ í˜¸ë¥¼ ë‚´ë³´ë‚´ëŠ” ë¶€ë¶„\n",
    "- **Synaptic terminals** : ì‹œëƒ…ìŠ¤(synapse) ë‰´ëŸ°ì˜ ì ‘í•©ë¶€, ë‹¤ë¥¸ ë‰´ëŸ°ìœ¼ë¡œ ë¶€í„° ì§§ì€ ì „ê¸° ìžê·¹ **ì‹ í˜¸**(signal)ë¥¼ ë°›ìŒ\n",
    "â€‹\n",
    "![](./images/neuron.png)\n",
    "\n",
    "ìš°ë¦¬ì˜ ë‡ŒëŠ” ìˆ˜ìƒëŒê¸°ì—ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ ë‹¤ìŒ ìˆ˜ìƒëŒê¸°ì˜ synapseì—ì„œ ì´ì „ ì •ë³´ë¥¼ í™œì„±í™” ìœ ë¬´ë¥¼ ê²°ì •í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.1.3 í¼ì…‰íŠ¸ë¡ \n",
    "í¼ì…‰íŠ¸ë¡ (Perceptron)ì€ Frank Rosenblattê°€ 1975ë…„ì— ì œì•ˆí•œ ì¸ê³µ ì‹ ê²½ë§ êµ¬ì¡° ì¤‘ í•˜ë‚˜ì´ë©°, ì´ í¼ì…‰íŠ¸ë¡ ì´ ë°”ë¡œ ì‹ ê²½ë§(ë”¥ëŸ¬ë‹)ì˜ ê¸°ì›ì´ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  í•  ìˆ˜ ìžˆë‹¤. í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•œ ìžì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì„œ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.\n",
    "\n",
    "í¼ì…‰íŠ¸ë¡ ì€ TLU(Threshold Logic Unit)ì´ë¼ëŠ” í˜•íƒœì˜ ë‰´ëŸ°ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì•„ëž˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ìž…ë ¥ê³¼ ì¶œë ¥ì´ ì–´ë–¤ ìˆ«ìžê³  ê°ê°ì˜ ìž…ë ¥ì— ê°ê° ê³ ìœ í•œ ê°€ì¤‘ì¹˜( ð– , weight)ê°€ ê³±í•´ì§„ë‹¤.\n",
    "ð‘§=ð‘¤1ð‘¥1+ð‘¤2ð‘¥2+â‹¯+ð‘¤ð‘›ð‘¥ð‘›=ð–ð‘‡â‹…ð—\n",
    " \n",
    "ê·¸ëŸ° ë‹¤ìŒ ê³„ì‚°ëœ í•©  ð‘§ ì— ê³„ë‹¨ í•¨ìˆ˜(step function)ë¥¼ ì ìš©í•˜ì—¬ ê²°ê³¼  â„Ž ë¥¼ ì¶œë ¥í•œë‹¤.\n",
    "![](./images/perceptron02.png)\n",
    "ê°€ì¤‘ì¹˜ì˜ í•©ê¹Œì§€ëŠ” ìš°ë¦¬ê°€ ê¸°ì¡´ì— ë°°ì› ë˜ ì„ í˜•í•¨ìˆ˜ì™€ ë™ì¼í•˜ë‹¤. í•˜ì§€ë§Œ ì•žì„œ ì‹ ê²½ë§ì„ í†µí•´ ë³´ì•˜ë˜\n",
    "ì‹œëƒ…ìŠ¤ ë¶€ë¶„ì¸ ê³„ë‹¨í•¨ìˆ˜ê°€ ì¡´ìž¬í•œë‹¤. ê³„ë‹¨í•¨ìˆ˜ëŠ” ê°€ì¤‘ì¹˜ í•©ì´ ì¼ì •ì¹˜ë¥¼ ë„˜ì„ ê²½ìš° í™œì„±í™” í•˜ë©°, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´\n",
    "ë¹„í™œì„±í™”ëœë‹¤. ê³„ë‹¨í•¨ìˆ˜ë¥¼ í†µí•´ ì´ì§„ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•˜ë‹¤.\n",
    "ì´ëŸ¬í•œ í¼ì…‰íŠ¸ë¡ ì„ 3ê°œë¡œ êµ¬ì„±í•˜ë©´ ì´ í¼ì…‰íŠ¸ë¡ ì€ ìƒ˜í”Œ ì„¸ ê°œì˜ í´ëž˜ìŠ¤(ë ˆì´ë¸”)ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìžˆëŠ” Multioutput Classifierì´ë‹¤.\n",
    "\n",
    "![](./images/multi-tlu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í¼ì…‰íŠ¸ë¡ ì˜ ì•½ì : XOR ë¬¸ì œ\n",
    "\n",
    "1969ë…„ Marvin Minskyì™€ Seymour PapertëŠ” 'í¼ì…‰íŠ¸ë¡ 'ì´ëž€ ë…¼ë¬¸ì—ì„œ í¼ì…‰íŠ¸ë¡ ì˜ ì‹¬ê°í•œ ì•½ì ì´ ìžˆë‹¤ëŠ” ê²ƒì„ ë³´ì˜€ëŠ”ë°, ê·¸ ì¤‘ì—ì„œë„ ê°€ìž¥ ê²°ì •ì ì¸ ê²ƒì€ ì„ í˜•ê²°í•©ì¸ í¼ì…‰íŠ¸ë¡ ì´ ë°°íƒ€ì  ë…¼ë¦¬í•©ì¸ XOR ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ì—ˆë‹¤. \n",
    "\n",
    "![or-vs-xor](./images/or-vs-xor.png)\n",
    "ë‹¨ì¸µìœ¼ë¡œ í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜•ì˜ ì¡°í•©ê³¼ ë™ì¼í•˜ë©° ì´ëŠ” xorê³¼ ê°™ì€ ê°„ë‹¨í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ì—ˆìœ¼ë©°,\n",
    "ì´ì— ë”°ë¼ì„œ SVMê³¼ RANDOMFORESTì™€ ê°™ì´ ë” ê°„ë‹¨í•˜ë©´ì„œ ê°•ë ¥í•œ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ì´ ì£¼ëª© ë°›ê³ , ANNì— ëŒ€í•œ í•œê³„ë¡œ ë°œì „ì´ ë©ˆì¶”ì—ˆë‹¤.\n",
    "\n",
    "\n",
    "ì´ë¥¼ ê³„ê¸°ë¡œ ì¸ê³µ ì‹ ê²½ë§ì€ ì•”í‘ê¸°ë¥¼ ë§žì´í•˜ê²Œ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ, ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì„ ì—¬ëŸ¬ê°œ ìŒ“ì•„ **ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ **(**MLP**, Multi-Layer Perceptron)ì„ í†µí•´ XOR ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆì—ˆë‹¤. \n",
    "ì•„ëž˜ì™€ ê°™ì´ Layerê°€ 1ê°œì—ì„œ 2ê°œë¡œ ëŠ˜ì–´ë‚¬ì„ ë•Œ, ë” ë³µìž¡í•œ ë¹„ì„ í˜•ì„±ì„ ê°€ì§ˆ ìˆ˜ ìžˆì—ˆê³  ì´ë¥¼ í†µí•´ XORë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆì—ˆë‹¤.\n",
    "![](./images/xor_gate05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—­ì „íŒŒë¥¼ í†µí•´ wì™€ bë¥¼ í•™ìŠµí• ìˆ˜ ìžˆìœ¼ë©°, \n",
    "ë‹¤ì¸µì˜ ë ˆì´ì–´ì™€ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ xorê³¼ ê°™ì´ ë³µìž¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆë‹¤ëŠ” ê²ƒì„ \n",
    "ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ë°°ì› ë‹¤.\n",
    "í•˜ì§€ë§Œ ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ë¬¸ì œì ì´ ë°œê²¬ë˜ì—ˆëŠ”ë° ê¸°ì¡´ê¹Œì§€ëŠ” sigmoidë¥¼ activation functionìœ¼ë¡œ \n",
    "ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ì´ì— ëŒ€í•´ì„œ layerë¥¼ ê¹Šê²Œ í•  ê²½ìš°\n",
    "vanishing gradientê°€ ë°œìƒí•˜ì˜€ë‹¤.\n",
    "vanishing gradientëŠ” ì—­ì „íŒŒ ê³¼ì •ì—ì„œ sigmoidì˜ ë¯¸ë¶„ì„ ì—¬ëŸ¬ë²ˆ ê±°ì¹˜ì–´, ì•žìª½ì— ìžˆëŠ”\n",
    "wê°€ updateê°€ ë˜ì§€ ì•Šê³  ë³€í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. \n",
    "ì‹ ê²½ë§ì€ ë§Žì€ layerë¥¼ í†µí•´ ë¹„ì„ í˜•ì„±ê³¼ ë³µìž¡í•œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìžˆëŠ” ê²ƒì´ ê°•ì ì„ ê°€ì§€ëŠ” ê²ƒì¸ë°\n",
    "ë¹ˆì„ í˜•ì„±ì„ ê°€ì§€ëŠ”ë° í•œê³„ë¥¼ ê°€ì§€ë¯€ë¡œ, ì‹ ê²½ë§ì— ëŒ€í•œ ê´€ì‹¬ì´ ì¤„ì–´ë“œëŠ” ê³„ê¸°ì˜€ë‹¤.\n",
    "![](./images/vanishing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í™œì„±í™” í•¨ìˆ˜ (activation function)\n",
    "\n",
    "ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì´ ìž˜ ë™ìž‘í•˜ê¸° ìœ„í•´ì„œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ì˜ êµ¬ì¡°ì— ë³€í™”ë¥¼ ì£¼ì—ˆëŠ” ë°, ê·¸ê²ƒì´ ë°”ë¡œ í™œì„±í™” í•¨ìˆ˜ ë¶€ë¶„ì—ì„œ ê³„ë‹¨ í•¨ìˆ˜ë¥¼ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜)ë¡œ ë°”ê¿”ì¤€ ê²ƒì´ë‹¤. ì´ë ‡ê²Œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ë°”ê¿”ì¤€ ì´ìœ ëŠ” ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì • í•´ì£¼ê¸° ìœ„í•´ ê·¸ëž˜ë””ì–¸íŠ¸, íŽ¸ë¯¸ë¶„ì„ ê³„ì‚°í•˜ê²Œ ë˜ëŠ”ë°, ê³„ë‹¨ í•¨ìˆ˜ëŠ” 0ì„ ê¸°ì¤€ìœ¼ë¡œ ê¸°ìš¸ê¸°ê°€ ì—†ëŠ” ì§ì„ ì´ë¯€ë¡œ ê·¸ëž˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì˜ë¯¸ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤(0ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶ˆì—°ì†ì´ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•œ ì´ìœ ë„ ìžˆë‹¤).  í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì•„ëž˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ ì™¸ì— ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.\n",
    "\n",
    "![](./images/activation02.png)\n",
    "\n",
    "\n",
    "\n",
    "ìœ„ì˜ ê·¸ë¦¼ì—ì„œ **ReLU** ë˜í•œ 0ì—ì„œ ì—°ì†ì´ì§€ë§Œ ì²©ì (ë¾°ì¡±í•œ ì )ì´ë¯€ë¡œ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. í•˜ì§€ë§Œ 0ë³´ë‹¤ í° ê²½ìš°ì—ëŠ” ë¯¸ë¶„ì„ ì ìš©í•˜ê³  0 ì´í•˜ì¸ ê°’ì—ëŠ” 0ì„ ì¤Œìœ¼ë¡œì¨ í•´ê²°í•  ìˆ˜ ìžˆë‹¤. ë˜í•œ, ReLUê°€ ì„±ëŠ¥ì´ ì¢‹ì„ ë¿ë§Œì•„ë‹ˆë¼ 'cs231n' ê°•ì˜ì—ì„œëŠ” ì‹¤ì œ ìƒë¬¼í•™ì ìœ¼ë¡œ ì‹œê·¸ëª¨ì´ë“œ ë³´ë‹¤ ê·¸ëŸ´ë“¯í•œ ìž‘ìš©ì„ í•œë‹¤ê³  í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"ìŠ¤í…\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2, label=\"ë¡œì§€ìŠ¤í‹±\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"í™œì„±í™” í•¨ìˆ˜\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(logit, z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.title(\"ë„í•¨ìˆ˜\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidì™€ ë¹„ìŠ·í•œ tanhì„ ë³´ì•˜ì„ë•Œ, ë„í•¨ìˆ˜ê°€ ëŒ€ë¶€ë¶„ 0 ì£¼ìœ„ì— ë¶„í¬í•˜ëŠ” ê²ƒì„ íŒŒì•…í•  ìˆ˜ ìžˆë‹¤.\n",
    "ì´ë ‡ë“¯ ê¸°ì¡´ì˜ sigmoidëŠ” ë¯¸ë¶„ê²°ê³¼ 0ì— ê·¼ì ‘í•˜ì˜€ìœ¼ë©°, layerê°€ ì—¬ëŸ¬ì¸µì´ ìžˆì„ ê²½ìš°\n",
    "ë¯¸ë¶„ê°’ì´ 0ì— ê·¼ì ‘í•˜ì—¬ vanishing gradientê°€ ë°œìƒí–ˆë‹¤ëŠ” ì•Œ ìˆ˜ ìžˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì†Œí”„íŠ¸ë§¥ìŠ¤(softmax) í•¨ìˆ˜\n",
    "\n",
    "**ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜**(softmax function)ëŠ” ì¶œë ¥ì¸µì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì´ë©°, ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "$$\n",
    "\\hat{y}_k = \\frac{ \\text{exp} \\left( \\mathbf{W}^{T} \\cdot \\mathbf{x} \\right)}{\\sum_{j=1}^{K}{\\text{exp} \\left( \\mathbf{W}^{T} \\cdot \\mathbf{x} \\right)}} \\quad (K=\\text{# of class})\n",
    "$$\n",
    "\n",
    "\n",
    "ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì˜ íŠ¹ì§•ì€ ì¶œë ¥ê°’ì˜ ì´í•©ì´ 1ì´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ê° ì¶œë ¥ ë‰´ëŸ°ì— ëŒ€í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ì˜ ì¶œë ¥ê°’ì€ ê° í´ëž˜ìŠ¤ì— ëŒ€ì‘í•˜ëŠ” ì¶”ì • í™•ë¥ ê°’ìœ¼ë¡œ ë³¼ ìˆ˜ ìžˆë‹¤.\n",
    "ì´í•©ì´ 1ë¡œ ì •ê·œí™”ë˜ëŠ” ìž¥ì ê³¼ ë™ì‹œì—, ìžì—°í•¨ìˆ˜ì˜ íŠ¹ì§•ìœ¼ë¡œ ì¸í•˜ì—¬, ê° í´ëž˜ìŠ¤ì˜ í™•ë¥ ì˜ ì°¨ì´ê°€ ì¦ê°€í•˜ë©°, ì´ëŠ” ë¹„ìš©í•¨ìˆ˜ì—ì„œ ë§žì¶˜ê²ƒì— ëŒ€í•œ ì •í™•ë„ëŠ” ë” ë†’ê²Œ íŒë‹¨í•˜ê³  í‹€ë¦° ê²ƒì˜ ì°¨ì´ëŠ” ë” í¬ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ìž¥ì ì„ ê°€ì§„ë‹¤.\n",
    "\n",
    "![](./images/softmax.png)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnistë°ì´í„°ë¡œ ann ì‹¤ìŠµ\n",
    "![]("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/mmmnnn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MNIST Dataset Load!\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# reshape : 28 x 28 -> 784\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "# split validation set\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################\n",
    "# layer params #\n",
    "################\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"inputs\")\n",
    "labels = tf.placeholder(tf.int32, shape=[None], name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(inputs=inputs, units=n_hidden1,\n",
    "                              activation=tf.nn.relu, name='hidden1',\n",
    "                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu, name='hidden2',\n",
    "                              kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Hyper-params #\n",
    "################\n",
    "learning_rate = 0.01\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(predictions=logits, targets=labels, k=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(train_op, feed_dict={inputs: X_batch,\n",
    "                                          labels: y_batch})\n",
    "        \n",
    "        acc_batch = accuracy.eval(feed_dict={inputs: X_batch, labels: y_batch})\n",
    "        acc_valid = accuracy.eval(feed_dict={inputs: X_valid, labels: y_valid})\n",
    "        print('epoch: {:03d}, bacth acc: {:.4f}, valid acc: {:.4f}'.format(epoch,\n",
    "                                                                          acc_batch,\n",
    "                                                                          acc_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
